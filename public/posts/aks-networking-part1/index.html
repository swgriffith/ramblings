<!doctype html>
<html lang="en-us">
  <head>
    <title>Aks Networking Part 1 - Kubenet on AKS // Steve Griffith</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.75.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Steve Griffth" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://www.stevegriffith.nyc/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-129005800-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Aks Networking Part 1 - Kubenet on AKS"/>
<meta name="twitter:description" content="Setup For this session we&rsquo;ll create a resource group with a Vnet, three subnets and two AKS Clusters.
Create Resource Group, Vnet and Subnets RG=NetworkLab LOC=eastus VNET_CIDR=&#34;10.220.0.0/16&#34; KUBENET_AKS_CIDR=&#34;10.220.1.0/24&#34; AZURECNI_AKS_CIDR=&#34;10.220.2.0/24&#34; SVC_LB_CIDR=&#34;10.220.3.0/24&#34; # Create Resource Group az group create -n $RG -l $LOC # Create Vnet az network vnet create \ -g $RG \ -n aksvnet \ --address-prefix $VNET_CIDR # Create Kubenet AKS Cluster Subnet az network vnet subnet create \  --resource-group $RG \  --vnet-name aksvnet \  --name kubenet \  --address-prefix $KUBENET_AKS_CIDR # Get the Kubenet Subnet ID KUBENET_SUBNET_ID=$(az network vnet show -g $RG -n aksvnet -o tsv --query &#34;subnets[?"/>
<meta name="twitter:site" content="@SteveGriffith"/>

    <meta property="og:title" content="Aks Networking Part 1 - Kubenet on AKS" />
<meta property="og:description" content="Setup For this session we&rsquo;ll create a resource group with a Vnet, three subnets and two AKS Clusters.
Create Resource Group, Vnet and Subnets RG=NetworkLab LOC=eastus VNET_CIDR=&#34;10.220.0.0/16&#34; KUBENET_AKS_CIDR=&#34;10.220.1.0/24&#34; AZURECNI_AKS_CIDR=&#34;10.220.2.0/24&#34; SVC_LB_CIDR=&#34;10.220.3.0/24&#34; # Create Resource Group az group create -n $RG -l $LOC # Create Vnet az network vnet create \ -g $RG \ -n aksvnet \ --address-prefix $VNET_CIDR # Create Kubenet AKS Cluster Subnet az network vnet subnet create \  --resource-group $RG \  --vnet-name aksvnet \  --name kubenet \  --address-prefix $KUBENET_AKS_CIDR # Get the Kubenet Subnet ID KUBENET_SUBNET_ID=$(az network vnet show -g $RG -n aksvnet -o tsv --query &#34;subnets[?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.stevegriffith.nyc/posts/aks-networking-part1/" />
<meta property="article:published_time" content="2020-11-06T13:56:29-05:00" />
<meta property="article:modified_time" content="2020-11-06T13:56:29-05:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://www.stevegriffith.nyc/"><img class="app-header-avatar" src="/images/me.jpg" alt="Steve Griffth" /></a>
      <h1>Steve Griffith</h1>
      <p>Microsoft Cloud Native Global Black Belt. This is my personal blog, so opinions are my own.</p>
      <div class="app-header-social">
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Aks Networking Part 1 - Kubenet on AKS</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Nov 6, 2020
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          10 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://www.stevegriffith.nyc/tags/azure/">azure</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubernetes/">kubernetes</a><a class="tag" href="https://www.stevegriffith.nyc/tags/networking/">networking</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubenet/">kubenet</a><a class="tag" href="https://www.stevegriffith.nyc/tags/cni/">cni</a><a class="tag" href="https://www.stevegriffith.nyc/tags/aks/">aks</a></div></div>
    </header>
    <div class="post-content">
      <h2 id="setup">Setup</h2>
<p>For this session we&rsquo;ll create a resource group with a Vnet, three subnets and two AKS Clusters.</p>
<h3 id="create-resource-group-vnet-and-subnets">Create Resource Group, Vnet and Subnets</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">RG<span style="color:#f92672">=</span>NetworkLab
LOC<span style="color:#f92672">=</span>eastus
VNET_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.0.0/16&#34;</span>
KUBENET_AKS_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.1.0/24&#34;</span>
AZURECNI_AKS_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.2.0/24&#34;</span>
SVC_LB_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.3.0/24&#34;</span>

<span style="color:#75715e"># Create Resource Group</span>
az group create -n $RG -l $LOC

<span style="color:#75715e"># Create Vnet</span>
az network vnet create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-g $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n aksvnet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--address-prefix $VNET_CIDR

<span style="color:#75715e"># Create Kubenet AKS Cluster Subnet</span>
az network vnet subnet create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vnet-name aksvnet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name kubenet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --address-prefix $KUBENET_AKS_CIDR

<span style="color:#75715e"># Get the Kubenet Subnet ID</span>
KUBENET_SUBNET_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az network vnet show -g $RG -n aksvnet -o tsv --query <span style="color:#e6db74">&#34;subnets[?name==&#39;kubenet&#39;].id&#34;</span><span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Create Azure CNI AKS Cluster Subnet</span>
az network vnet subnet create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vnet-name aksvnet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name azurecni <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --address-prefix $AZURECNI_AKS_CIDR

<span style="color:#75715e"># Create the subnet for Kubernetes Service Load Balancers</span>
az network vnet subnet create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vnet-name aksvnet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name services <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --address-prefix $SVC_LB_CIDR
</code></pre></div><h3 id="pod-and-service-cidr-sizes">Pod and Service CIDR Sizes</h3>
<p>Before we create the cluster it&rsquo;s worth discussing the size of the address space (aka. cidr) for our pods and services. In both cases, when we&rsquo;re using kubenet the address range will be an &lsquo;overlay&rsquo; network, which we&rsquo;ll explain further as we go through this guide. Since we&rsquo;ll generally have more pods than services in a given cluster, you can make the services cidr smaller than the pod cidr. For this walk-through, however, I&rsquo;m just leaving them both as /16.</p>
<p>When we create services of type &lsquo;LoadBalancer&rsquo; this will trigger the creation of an Azure Load Balancer either in the cluster subnet (by default), or optionally in a user specified subnet, at which point those services will consume private address space that you will likely want to conserve. Setting the target subnet for the creation of services of type LoadBalancer is a good practice to ensure better control of overall IP usage and to enable better control of routing and network security rules. See the Azure doc on <a href="https://docs.microsoft.com/en-us/azure/aks/internal-lb#specify-a-different-subnet">AKS Internal LBs</a> to see how to apply a target subnet.</p>
<h3 id="create-the-kubenet-aks-cluster">Create the Kubenet AKS Cluster</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">######################################</span>
<span style="color:#75715e"># Create the Kubenet AKS Cluster</span>
<span style="color:#75715e"># Note: We set a pod cidr, service cidr</span>
<span style="color:#75715e"># and dns service ip for demonstration</span>
<span style="color:#75715e"># purposes, however these are optional</span>
<span style="color:#75715e">#######################################</span>
az aks create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-g $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n kubenet-cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--network-plugin kubenet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--vnet-subnet-id $KUBENET_SUBNET_ID <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--pod-cidr <span style="color:#e6db74">&#34;10.100.0.0/16&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--service-cidr <span style="color:#e6db74">&#34;10.200.0.0/16&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--dns-service-ip <span style="color:#e6db74">&#34;10.200.0.10&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--enable-managed-identity

<span style="color:#75715e"># Get Credentials</span>
az aks get-credentials -g $RG -n kubenet-cluster

<span style="color:#75715e"># Get the deployment manifest</span>
wget https://raw.githubusercontent.com/swgriffith/azure-guides/master/networking-overview/nginx.yaml

<span style="color:#75715e"># Deploy 3 Nginx Pods across 3 nodes</span>
kubectl apply -f nginx.yaml

<span style="color:#75715e"># View the Services and pods</span>
kubectl get svc
kubectl get pods -o wide --sort-by<span style="color:#f92672">=</span>.spec.nodeName <span style="color:#75715e"># Sorted by node name</span>
</code></pre></div><h3 id="pod-and-service-cidr-behavior">Pod and Service CIDR behavior</h3>
<p>Notice from your get svc and pods calls that the private IP addresses are from the pod and service cidr ranges you specified at cluster creation, and not from you subnet cidr.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Subnet CIDR from network creation</span>
KUBENET_AKS_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.1.0/24&#34;</span>

<span style="color:#75715e"># CIDR Values from &#39;az aks create&#39;</span>
--pod-cidr <span style="color:#e6db74">&#34;10.100.0.0/16&#34;</span>
--service-cidr <span style="color:#e6db74">&#34;10.200.0.0/16&#34;</span>
</code></pre></div><p>Fig. 1
<img src="/azure-networking/kubenetsvcspods.png" alt="Services and Pods"></p>
<p>To dig a bit deeper, lets ssh into the node and explore the network configuration. For this we&rsquo;ll use <a href="https://github.com/yokawasa/kubectl-plugin-ssh-jump/blob/master/README.md">ssh-jump</a> but there are various other options, including using privileged containers. If you do ssh to a node, you&rsquo;ll need to <a href="https://docs.microsoft.com/en-us/azure/aks/ssh">set up ssh access</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the managed cluster resource group and scale set names</span>
CLUSTER_RESOURCE_GROUP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az aks show --resource-group $RG --name kubenet-cluster --query nodeResourceGroup -o tsv<span style="color:#66d9ef">)</span>
SCALE_SET_NAME<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az vmss list --resource-group $CLUSTER_RESOURCE_GROUP --query <span style="color:#e6db74">&#34;[0].name&#34;</span> -o tsv<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Add your local public key to the VMSS to enable ssh access</span>
az vmss extension set  <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $CLUSTER_RESOURCE_GROUP <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vmss-name $SCALE_SET_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name VMAccessForLinux <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --publisher Microsoft.OSTCExtensions <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --version 1.4 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --protected-settings <span style="color:#e6db74">&#34;{\&#34;username\&#34;:\&#34;azureuser\&#34;, \&#34;ssh_key\&#34;:\&#34;</span><span style="color:#66d9ef">$(</span>cat ~/.ssh/id_rsa.pub<span style="color:#66d9ef">)</span><span style="color:#e6db74">\&#34;}&#34;</span>

az vmss update-instances --instance-ids <span style="color:#e6db74">&#39;*&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $CLUSTER_RESOURCE_GROUP <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name $SCALE_SET_NAME

<span style="color:#75715e"># Get a node name and ssh-jump to it</span>
<span style="color:#75715e"># Make sure you jump to a node where one of your nginx pods is running</span>
kubectl get nodes
NAME                                STATUS   ROLES   AGE    VERSION
aks-nodepool1-27511634-vmss000000   Ready    agent   4d3h   v1.17.11
aks-nodepool1-27511634-vmss000001   Ready    agent   4d3h   v1.17.11
aks-nodepool1-27511634-vmss000002   Ready    agent   4d3h   v1.17.11

<span style="color:#75715e"># Use ssh-jump to access a node. Note that it may take a minute for the jump pod to come online</span>
kubectl ssh-jump &lt;Insert a node name&gt;

<span style="color:#75715e"># Get the docker id for the nginx pod</span>
docker ps|grep nginx
d01940d20034        nginx                                          <span style="color:#e6db74">&#34;/docker-entrypoint.â€¦&#34;</span>   <span style="color:#ae81ff">24</span> minutes ago      Up <span style="color:#ae81ff">24</span> minutes                           k8s_nginx_nginx-7cf567cc7-8879g_default_33aa572a-8816-4635-b9c4-be315b270f27_0
660dcdb7ed1c        mcr.microsoft.com/oss/kubernetes/pause:1.3.1   <span style="color:#e6db74">&#34;/pause&#34;</span>                 <span style="color:#ae81ff">24</span> minutes ago      Up <span style="color:#ae81ff">24</span> minutes                           k8s_POD_nginx-7cf567cc7-8879g_default_33aa572a-8816-4635-b9c4-be315b270f27_0
</code></pre></div><p>Ok, wait&hellip;.why do we have two containers for this single nginx pod? Go check out the <a href="https://www.ianlewis.org/en/almighty-pause-container">the almighty pause container</a>. In short, the other container is the &lsquo;/pause&rsquo; container, which is the parent container for all containers within a given Kubernetes pod.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the pid for your container</span>
docker inspect --format <span style="color:#e6db74">&#39;{{ .State.Pid }}&#39;</span> 660dcdb7ed1c
<span style="color:#ae81ff">14219</span>

<span style="color:#75715e"># List the network interfaces for the pid</span>
sudo nsenter -t <span style="color:#ae81ff">14219</span> -n ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen <span style="color:#ae81ff">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if19: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noqueue state UP group default
    link/ether a6:89:88:b2:e5:a6 brd ff:ff:ff:ff:ff:ff link-netnsid <span style="color:#ae81ff">0</span>
    inet 10.100.1.14/24 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre></div><p>Notice the eth0 is @if19, meaning its attached to interface 19, but what is that? If we take a look at the host machine interfaces we can see that there is an interface with the index of 19 named &ldquo;vethd3b9c108@if3&rdquo; as you can see in the image below, @if3 and @if19 are the link between the container network interface and the host network interface, which happens to be a veth link.</p>
<p>Fig 2.
<img src="/azure-networking/vethlink.png" alt="veth link"></p>
<p>Ok, so now we know how each container is connected to the a virtual ethernet interface on the host, but where does it get it&rsquo;s IP and how does it communicate out of the node? Our first hint is the &lsquo;cbr0&rsquo; name listed in the &lsquo;ip addr&rsquo; output for our veth.  Checking out the Kubernetes docs on <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#kubenet">Kubenet</a>, we know that cbr0 is the bridge network created and managed by kubenet. We can see the interface in our &lsquo;ip addr&rsquo; output. Also notice that the inet value for cbr0 is 10.100.1.1/24, which happens to be our pod cidr! So cbr0 is the bridge network that the veth links are joined to. We can confirm this by using the brctl command from bridge-utils. Notice all of our veth interfaces attached to cbr0.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Install the bridge-utils package</span>
sudo apt update
sudo apt install bridge-utils

<span style="color:#75715e"># Show the bridge networks on the server</span>
brctl show
bridge name bridge id          STP enabled  interfaces
cbr0        8000.160c0cac5660  no           veth9423965c
                                            vetha1065ff2
                                            vetha33f314e
                                            vethcd3ab008
                                            vethd3b9c108
</code></pre></div><p>Further, if we look at the routes defined on our machine we can see that any traffic destined for our pod cidr should be sent to that cbr0 bridge interface, and the traffic leaving our cbr0 bridge should go to the default route (0.0.0.0)&hellip;which uses eth0 and points to our network gateway address (10.220.1.1).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get Routes</span>
route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.220.1.1      0.0.0.0         UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
10.100.1.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> cbr0
10.220.1.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
168.63.129.16   10.220.1.1      255.255.255.255 UGH   <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
169.254.169.254 10.220.1.1      255.255.255.255 UGH   <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> docker0
</code></pre></div><p>FINALLY, the bridge network has brought us to the NIC of our Azure node (eth0). So, now we have the network wiring in place to get packets from outside of our node to a given container, within a pod, and the wiring to get traffic out of a container and pod out through the node network interface card. There&rsquo;s more to cover as our packet traverses that path, in particular how Kubernetes uses iptables to direct traffic flow, but lets hold off on how iptables come into play until after we look at Azure CNI so we can compare how the Kubenet and CNI wiring differ.</p>
<blockquote>
<p><strong>NOTE:</strong> In this walk through we are NOT covering the impact of network policy on the above. There are some key key changes to the above introduced by a network policy plugin. For more information I created this side bar post on <a href="/posts/bridge-vs-transparent/">bridge vs. transparent mode</a> you can check out.</p>
</blockquote>
<p>We still haven&rsquo;t seen how traffic from a container in one pod can reach a container in a pod on another node. This is one of the fundamental ways that Azure Kubernetes Service with the kubenet plugin differs from AKS with Azure CNI. Node to node traffic is directed by an Azure Route table. Before we look at the route table, one thing to know is that traffic between pods does not go through SNAT (Source NAT). That means that when a pod sends traffic to another pod, it retains it&rsquo;s pod ip.</p>
<p>I know I said we&rsquo;d cover iptables later, but just fyi&hellip;this is the set of rules that ensure packets originating from our pod cidr dont get SNAT&rsquo;d to the node IP address. Notice the !10.100.0.0/16 for destination, meaning &lsquo;NOT 10.100.0.0/16&rsquo; aka &lsquo;NOT our pod cidr&rsquo;. That rule is saying that SNAT should ONLY happen for packets NOT destined for our cluster&rsquo;s pod cidr.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Run iptables for the &#39;nat&#39; table pulling the POSTROUTING chain...and do some formatting to make more pretty</span>
sudo iptables -t nat -nL POSTROUTING --line-numbers
Chain POSTROUTING <span style="color:#f92672">(</span>policy ACCEPT<span style="color:#f92672">)</span>
num  target     prot opt source               destination
<span style="color:#ae81ff">1</span>    KUBE-POSTROUTING  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */
<span style="color:#ae81ff">2</span>    MASQUERADE  all  --  172.17.0.0/16        0.0.0.0/0
<span style="color:#ae81ff">3</span>    MASQUERADE  all  --  0.0.0.0/0           !10.100.0.0/16        /* kubenet: SNAT <span style="color:#66d9ef">for</span> outbound traffic from cluster */ ADDRTYPE match dst-type !LOCAL
</code></pre></div><h3 id="azure-route-table">Azure Route Table</h3>
<p>When traffic is leaving our node it can be destined for:</p>
<ol>
<li>Another network</li>
<li>A node in our current network</li>
<li>A pod in a node in our current network</li>
</ol>
<p>For 1 &amp; 2, we already saw above that our pod traffic will SNAT to the node IP address and will just go on their way along to their destination. For 3, however, the AKS kubenet implementation has an Azure Route Table that takes over. This route table is what tells Azure what node to route that pod traffic to. When nodes are added to an AKS kubenet cluster, the pod cidr is split into a /24 for each node.</p>
<p><img src="/azure-networking/routetable.png" alt="aks kubenet route table"></p>
<p>As you can see below, any traffic destined to pods in the 10.100.1.0/24 cidr will next hop to 10.220.1.4. Sure enough, if I look at the pods on that node I can see that they all have ips in that 10.100.1.0/24 range.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get nodes to see ips</span>
kubectl get nodes -o wide
NAME                                STATUS   ROLES   AGE     VERSION    INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
aks-nodepool1-27511634-vmss000000   Ready    agent   5d23h   v1.17.11   10.220.1.4    &lt;none&gt;        Ubuntu 16.04.7 LTS   4.15.0-1096-azure   docker://19.3.12
aks-nodepool1-27511634-vmss000001   Ready    agent   5d23h   v1.17.11   10.220.1.5    &lt;none&gt;        Ubuntu 16.04.7 LTS   4.15.0-1096-azure   docker://19.3.12
aks-nodepool1-27511634-vmss000002   Ready    agent   5d23h   v1.17.11   10.220.1.6    &lt;none&gt;        Ubuntu 16.04.7 LTS   4.15.0-1096-azure   docker://19.3.12

<span style="color:#75715e"># Get pods for the node with ip 10.220.1.4 (aks-nodepool1-27511634-vmss000000)</span>
kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName<span style="color:#f92672">=</span>aks-nodepool1-27511634-vmss000000
NAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE     IP            NODE                                NOMINATED NODE   READINESS GATES
default       nginx-7cf567cc7-bnvk9                        1/1     Running   <span style="color:#ae81ff">0</span>          34m     10.100.1.18   aks-nodepool1-27511634-vmss000000   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-869cb84759-vdh55                     1/1     Running   <span style="color:#ae81ff">0</span>          5d23h   10.100.1.5    aks-nodepool1-27511634-vmss000000   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-autoscaler-5b867494f-25vlb           1/1     Running   <span style="color:#ae81ff">5</span>          6d      10.100.1.3    aks-nodepool1-27511634-vmss000000   &lt;none&gt;           &lt;none&gt;
kube-system   dashboard-metrics-scraper-5ddb5bf5c8-ph4vs   1/1     Running   <span style="color:#ae81ff">0</span>          6d      10.100.1.4    aks-nodepool1-27511634-vmss000000   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-2n62m                             1/1     Running   <span style="color:#ae81ff">0</span>          5d23h   10.220.1.4    aks-nodepool1-27511634-vmss000000   &lt;none&gt;           &lt;none&gt;
</code></pre></div><p><strong>Note:</strong> Ignore the kube-proxy pod above, which has an ip of 10.220.1.4, which is the node ip. If you take a look at the definition of that pod you&rsquo;ll see that it attaches to the host network (<em>kubectl get pod kube-proxy-2n62m -n kube-system -o yaml|grep hostNetwork</em>)</p>
<h3 id="next">Next</h3>
<p>Now that we have a good idea of how kubenet works in AKS, lets have a look at Azure CNI</p>
<p><a href="/posts/aks-networking-part2/">Azure CNI</a></p>
<h3 id="big-picture">Big Picture</h3>
<p>Fig 3.
<img src="/azure-networking/kubenet-wiring.JPG" alt="kubenet wiring"></p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://azuregulfblog.files.wordpress.com/2019/04/aks_basicnetwork_technicalpaper.pdf">Understanding Azure Kubernetes Service Basic Networking</a></li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
