<!doctype html>
<html lang="en-us">
  <head>
    <title>Aks Networking Iptables in AKS // Steve Griffith</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.83.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Steve Griffth" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://www.stevegriffith.nyc/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-129005800-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Aks Networking Iptables in AKS"/>
<meta name="twitter:description" content="Overview We&rsquo;ve seen the network wiring for both kubenet and Azure CNI, so now we understand the core plumbing used to move packets around within an AKS cluster. There is one more layer that comes into play, however. As packets arrive on a host, the linux kernel will pass them through iptables to apply filtering (ex. Firewalls) and routing rules. Today, iptables is the default implementation for AKS in cluster routing."/>
<meta name="twitter:site" content="@SteveGriffith"/>

    <meta property="og:title" content="Aks Networking Iptables in AKS" />
<meta property="og:description" content="Overview We&rsquo;ve seen the network wiring for both kubenet and Azure CNI, so now we understand the core plumbing used to move packets around within an AKS cluster. There is one more layer that comes into play, however. As packets arrive on a host, the linux kernel will pass them through iptables to apply filtering (ex. Firewalls) and routing rules. Today, iptables is the default implementation for AKS in cluster routing." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.stevegriffith.nyc/posts/aks-networking-iptables/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-06T13:56:40-05:00" />
<meta property="article:modified_time" content="2020-11-06T13:56:40-05:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://www.stevegriffith.nyc/"><img class="app-header-avatar" src="/images/me.jpg" alt="Steve Griffth" /></a>
      <h1>Steve Griffith</h1>
      <p>Microsoft Cloud Native Global Black Belt. This is my personal blog, so opinions are my own.</p>
      <div class="app-header-social">
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Aks Networking Iptables in AKS</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Nov 6, 2020
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          14 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://www.stevegriffith.nyc/tags/azure/">azure</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubernetes/">kubernetes</a><a class="tag" href="https://www.stevegriffith.nyc/tags/networking/">networking</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubenet/">kubenet</a><a class="tag" href="https://www.stevegriffith.nyc/tags/cni/">cni</a><a class="tag" href="https://www.stevegriffith.nyc/tags/aks/">aks</a></div></div>
    </header>
    <div class="post-content">
      <h2 id="overview">Overview</h2>
<p>We&rsquo;ve seen the network wiring for both <a href="/posts/aks-networking-part1/">kubenet</a> and <a href="/posts/aks-networking-part2/">Azure CNI</a>, so now we understand the core plumbing used to move packets around within an AKS cluster. There is one more layer that comes into play, however. As packets arrive on a host, the linux kernel will pass them through iptables to apply filtering (ex. Firewalls) and routing rules. Today, iptables is the default implementation for AKS in cluster routing. IPVS has been considered, as noted on the <a href="/posts/aks-networking/">intro doc</a>, but as of yet there hasn&rsquo;t been enough clear need to outweigh the stability and maturity of iptables. Check out AKS issue <a href="https://github.com/Azure/AKS/issues/1846">#1846</a> for details, and to share your thoughts.</p>
<p>We&rsquo;re not going to go deep into how iptables work in this discussion, but there are plenty of good resources you can use to get up to speed at various levels of detail. I personally always love the arch linux docs when it comes to linux feature &amp; utility explanations. Check out <a href="https://wiki.archlinux.org/index.php/iptables">this</a> guide from the arch linux project.</p>
<h2 id="iptables-in-kubenet">iptables in kubenet</h2>
<p>Lets start by jumping back to our kubenet cluster, connecting to a node over ssh, and taking a look at the iptables entries over there. I&rsquo;d recommend you read up on iptables and then run the following two commands and start to work through the chains to see if you can figure out whats going on. We&rsquo;ll only be reading these tables, not trying to apply any changes.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># List the chains and rules at a high level</span>
sudo iptables -nvL

<span style="color:#75715e"># List the chains and rules associated with the nat table</span>
sudo iptables -t nat -nvL
</code></pre></div><p>As you ran through the output of the &lsquo;nat&rsquo; table rules, a few things may have jumped out at you.</p>
<ol>
<li>
<p>We seem to have a KUBE-SERVICE chain that contains all of the services on our cluster. Check out the end of line comments for each rule and you should be able to see the specific service name.</p>
</li>
<li>
<p>Our KUBE-SERVICE rules reference a KUBE-SVC-XXXX chain with it&rsquo;s own rules.</p>
</li>
<li>
<p>The individual rules within the KUBE-SVC chain point to a KUBE-SEP-XXXXX chain.</p>
</li>
<li>
<p>The KUBE-SEP-XXXX chain have a rule that points to the ip, port and protocol we exposed from our container</p>
</li>
</ol>
<p>Let&rsquo;s walk through this.</p>
<h3 id="kube-services">KUBE-SERVICES</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the nat table chains and rules</span>
sudo iptables -t nat -nvL KUBE-SERVICES

Chain KUBE-SERVICES <span style="color:#f92672">(</span><span style="color:#ae81ff">2</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.100.0.0/16        10.200.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.200.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  udp  --  *      *      !10.100.0.0/16        10.200.0.10          /* kube-system/kube-dns:dns cluster IP */ udp dpt:53
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.200.0.10          /* kube-system/kube-dns:dns cluster IP */ udp dpt:53
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.100.0.0/16        10.200.0.10          /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.200.0.10          /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.100.0.0/16        10.200.184.192       /* default/nginx: cluster IP */ tcp dpt:80
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  *      *       0.0.0.0/0            10.200.184.192       /* default/nginx: cluster IP */ tcp dpt:80
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.100.0.0/16        10.200.219.195       /* kube-system/metrics-server: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-LC5QY66VUV2HJ6WZ  tcp  --  *      *       0.0.0.0/0            10.200.219.195       /* kube-system/metrics-server: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.100.0.0/16        10.200.200.30        /* kube-system/kubernetes-dashboard: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-XGLOHA7QRQ3V22RZ  tcp  --  *      *       0.0.0.0/0            10.200.200.30        /* kube-system/kubernetes-dashboard: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.100.0.0/16        10.200.143.153       /* kube-system/dashboard-metrics-scraper: cluster IP */ tcp dpt:8000
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-O33EAQYCTNTKHSTD  tcp  --  *      *       0.0.0.0/0            10.200.143.153       /* kube-system/dashboard-metrics-scraper: cluster IP */ tcp dpt:8000
    <span style="color:#ae81ff">9</span>   <span style="color:#ae81ff">540</span> KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL

<span style="color:#75715e"># Grab the rules just for the nginx service</span>
sudo iptables -t nat -nvL KUBE-SERVICES|grep nginx
<span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.100.0.0/16        10.200.184.192       /* default/nginx: cluster IP */ tcp dpt:80
<span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  *      *       0.0.0.0/0            10.200.184.192       /* default/nginx: cluster IP */ tcp dpt:80
</code></pre></div><p>In the above output, as already mentioned you&rsquo;ll see that we have a rule in the KUBE-SERVICES chain for each service in our cluster.  That&rsquo;s right&hellip;.cluster level, not node level. You can see that by looking at the code comment for each, which shows the namespace and service name. If we look specifically at the nginx service, you can see that we have two rules.</p>
<ol>
<li>
<p><strong>KUBE-MARK-MASQ:</strong> If you look closely you can see that this rule checks if the source is NOT 10.100.0.0/16, which happens to be our pod cidr, and then sends that traffic to the KUBE-MARK-MASQ chain. This is where packets have a mark applied to them to indicate they should go through Source NAT.</p>
</li>
<li>
<p><strong>KUBE-SVC-XXXX:</strong> This is the chain that handles load balancing of the traffic across multiple backend pods</p>
</li>
</ol>
<h3 id="kube-svc-xxxx">KUBE-SVC-XXXX</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Using the name of our nginx KUBE-SVC chain, lets pull that detail</span>
sudo iptables -t nat -nvL KUBE-SVC-4N57TFCL4MD7ZTDA

Chain KUBE-SVC-4N57TFCL4MD7ZTDA <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-66QNMC7FITAI6UHV  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.33333333349
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-D27HGEPMBQMIMSQA  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-CP6YF2KE7E2OKMTG  all  --  *      *       0.0.0.0/0            0.0.0.0/0
</code></pre></div><p>Looking at the above, we can see that we have three rules, each pointing to a chain called &lsquo;KUBE-SEP-XXXX&rsquo;. We also see a probability added to the end of each rule. This is how the service traffic will be load balanced. These rules execute in order, so it looks like the following:</p>
<ol>
<li>
<p>Hit the first rule which says to send the packet to the KUBE-SEP-66QNMC7FITAI6UHV chain with a probability of 0.33333333349 (33%)</p>
</li>
<li>
<p>If the the packet wasnt sent to KUBE-SEP-66QNMC7FITAI6UHV then this rule says to apply another probability (50 % now because there are only two pods left) which will send to KUBE-SEP-D27HGEPMBQMIMSQA</p>
</li>
<li>
<p>If neither of the above probabilities hit, send the rest of the traffic to KUBE-SEP-CP6YF2KE7E2OKMTG</p>
</li>
</ol>
<p>If we were to scale up the service, we can see this chain get adjusted to a new set of probabilties.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Scale down to two</span>
<span style="color:#75715e"># Note: Since this deployment uses topology constraints to span nodes</span>
<span style="color:#75715e"># if you scale up on a three node cluster your new pod will go &#39;pending&#39;</span>
<span style="color:#75715e"># so we&#39;ll scale down for now</span>
kubectl scale deployment nginx --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>

<span style="color:#75715e"># Grap the iptaples for the KUBE-SVC chain again</span>
sudo iptables -t nat -nvL KUBE-SVC-4N57TFCL4MD7ZTDA

Chain KUBE-SVC-4N57TFCL4MD7ZTDA <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-66QNMC7FITAI6UHV  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-CP6YF2KE7E2OKMTG  all  --  *      *       0.0.0.0/0            0.0.0.0/0
</code></pre></div><p>Notice in the above that we only have two rules now with a 50% probability applied on the first rule?</p>
<blockquote>
<p><strong>Note:</strong> The above demonstrates the reason many people look towards IPVS, Cillium, and other network plugins. Probability based routing isnt always ideal. You can modify this behavior a bit by modifying the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">External Traffic Policy</a>. You can also look at the more advanced routing options provided by an ingress controller (ex. Nginx <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-nginx-load-balancing">load-balance</a> annotation)</p>
</blockquote>
<h3 id="kube-sep-xxx">KUBE-SEP-XXX</h3>
<p>So now lets check out the KUBE-SEP chain. Not exactly sure what &lsquo;SEP&rsquo; stands for, but one of these days I&rsquo;ll dig into the docs and find it. Feel free to PR the details there. Let&rsquo;s have a look.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Grab one of the KUBE-SEP-XXXX names from the KUBE-SVC-XXXX chain and then pull the chain details</span>
sudo iptables -t nat -nvL KUBE-SEP-66QNMC7FITAI6UHV

Chain KUBE-SEP-66QNMC7FITAI6UHV <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  all  --  *      *       10.100.0.9           0.0.0.0/0
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.100.0.9:80
</code></pre></div><p>In the above, we can finally see the last two rules applied.</p>
<ol>
<li>
<p>KUBE-MARK-MASQ - Again see that outbound traffic from the pod IP (10.100.0.9) should go through the KUBE-MARK-MASQ chain were SNAT will take place for destination 0.0.0.0/0.</p>
</li>
<li>
<p>DNAT - This is where the packet IP is tweaked to finally provide the target IP for the pod. In this case we&rsquo;ll send the traffic to the pod with an ip of 10.100.0.9 at port 80 over tcp. Looking below we can see that the ip referenced in this rule will send traffic to the pod named &lsquo;nginx-7cf567cc7-8bt8r&rsquo;</p>
</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pods -o wide

NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE                                NOMINATED NODE   READINESS GATES
nginx-7cf567cc7-8bt8r   1/1     Running   <span style="color:#ae81ff">0</span>          3h22m   10.100.0.9   aks-nodepool1-27511634-vmss000000   &lt;none&gt;           &lt;none&gt;
nginx-7cf567cc7-jnxp4   1/1     Running   <span style="color:#ae81ff">0</span>          3h22m   10.100.2.2   aks-nodepool1-27511634-vmss000001   &lt;none&gt;           &lt;none&gt;
</code></pre></div><h2 id="iptables-in-azure-cni">iptables in Azure CNI</h2>
<p>So we&rsquo;ve seen how iptables handles traffic for pods in kubenet, so lets run through the same path for an Azure CNI node. Go ahead and ssh to one of your Azure CNI cluster nodes and take a look at the high level rules like we did for kubenet, and then we&rsquo;ll walk through at a lower level.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># List the chains and rules at a high level</span>
sudo iptables -nvL

<span style="color:#75715e"># List the chains and rules associated with the nat table</span>
sudo iptables -t nat -nvL
</code></pre></div><p>You&rsquo;ll see after running the above, that overall things look pretty similar. There is one additional chain called IP-MASQ-AGENT that we should take a look at in a bit.</p>
<h3 id="kube-services-1">KUBE-SERVICES</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the nat table chains and rules</span>
sudo iptables -t nat -nvL KUBE-SERVICES

Chain KUBE-SERVICES <span style="color:#f92672">(</span><span style="color:#ae81ff">2</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.220.2.0/24        10.200.211.157       /* default/nginx: cluster IP */ tcp dpt:80
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  *      *       0.0.0.0/0            10.200.211.157       /* default/nginx: cluster IP */ tcp dpt:80
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  udp  --  *      *      !10.220.2.0/24        10.200.0.10          /* kube-system/kube-dns:dns cluster IP */ udp dpt:53
    <span style="color:#ae81ff">6</span>   <span style="color:#ae81ff">628</span> KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.200.0.10          /* kube-system/kube-dns:dns cluster IP */ udp dpt:53
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.220.2.0/24        10.200.0.10          /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.200.0.10          /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.220.2.0/24        10.200.239.189       /* kube-system/metrics-server: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-LC5QY66VUV2HJ6WZ  tcp  --  *      *       0.0.0.0/0            10.200.239.189       /* kube-system/metrics-server: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.220.2.0/24        10.200.75.15         /* kube-system/kubernetes-dashboard: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-XGLOHA7QRQ3V22RZ  tcp  --  *      *       0.0.0.0/0            10.200.75.15         /* kube-system/kubernetes-dashboard: cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.220.2.0/24        10.200.191.59        /* kube-system/dashboard-metrics-scraper: cluster IP */ tcp dpt:8000
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-O33EAQYCTNTKHSTD  tcp  --  *      *       0.0.0.0/0            10.200.191.59        /* kube-system/dashboard-metrics-scraper: cluster IP */ tcp dpt:8000
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.220.2.0/24        10.200.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.200.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
   <span style="color:#ae81ff">20</span>  <span style="color:#ae81ff">1200</span> KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL

<span style="color:#75715e"># Grab the rules just for the nginx service</span>
sudo iptables -t nat -nvL KUBE-SERVICES|grep nginx

<span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.220.2.0/24        10.200.211.157       /* default/nginx: cluster IP */ tcp dpt:80
<span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  *      *       0.0.0.0/0            10.200.211.157       /* default/nginx: cluster IP */ tcp dpt:80
</code></pre></div><p>So this looks pretty much exactly the same. Looking at the rules we have&hellip;.</p>
<ol>
<li>
<p><strong>KUBE-MARK-MASQ:</strong> Yet again, we see that if traffic is coming from a location other than the pod cidr, which in the Azure CNI case is the same as the subnet cidr&hellip;.that traffic will get sent to the KUBE-MARK-MASQ chain, which as mentioned above, will mark the packet so that it can go through Source NAT later.</p>
</li>
<li>
<p><strong>KUBE-SVC-XXXX:</strong> This rule, as with kubenet, will send any traffic destined for our service IP address to the KUBE-SVC-XXXX chain.</p>
</li>
</ol>
<h3 id="kube-svc-xxxx-1">KUBE-SVC-XXXX</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Using the name of our nginx KUBE-SVC chain, lets pull that detail</span>
sudo iptables -t nat -nvL KUBE-SVC-4N57TFCL4MD7ZTDA

Chain KUBE-SVC-4N57TFCL4MD7ZTDA <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-3OT3PH67SPCSYRVE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.33333333349
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-E4OJANFAINTG7AV5  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-SEP-JNUYOJDTS3SCVUAH  all  --  *      *       0.0.0.0/0            0.0.0.0/0
</code></pre></div><p>The behavior of the KUBE-SVC-XXXX chain is identical to the same chain in kubenet, so I wont run through that again.</p>
<h3 id="kube-sep-xxxx">KUBE-SEP-XXXX</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Using the name of one of the KUBE-SEP-XXXX chains, lets pull that detail</span>
sudo iptables -t nat -nvL KUBE-SEP-3OT3PH67SPCSYRVE

Chain KUBE-SEP-3OT3PH67SPCSYRVE <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> KUBE-MARK-MASQ  all  --  *      *       10.220.2.28          0.0.0.0/0
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.220.2.28:80
</code></pre></div><p>Again, this is effectively identical to the path taken for kubenet, so no reason to go over that again.</p>
<p>I did mention above that there is one additional chain we should look at that is not present in kubenet. It&rsquo;s called IP-MASQ-AGENT and it&rsquo;s triggered in the POST-ROUTINGas one of the very last steps as packets are leaving the cluster. Lets check this one out.</p>
<h3 id="ip-masq-agent">IP-MASQ-AGENT</h3>
<p>As noted above the IP-MASQ-AGENT chain is called by the POSTROUTING chain, as you can see below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo iptables -t nat -nvL POSTROUTING

Chain POSTROUTING <span style="color:#f92672">(</span>policy ACCEPT <span style="color:#ae81ff">0</span> packets, <span style="color:#ae81ff">0</span> bytes<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
<span style="color:#ae81ff">14204</span>  860K KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0
<span style="color:#ae81ff">14190</span>  860K IP-MASQ-AGENT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* ip-masq-agent: ensure nat POSTROUTING directs all non-LOCAL destination traffic to our custom IP-MASQ-AGENT chain */ ADDRTYPE match dst-type !LOCAL
</code></pre></div><p>Now lets look at what it does.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo iptables -t nat -nvL IP-MASQ-AGENT

Chain IP-MASQ-AGENT <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> RETURN     all  --  *      *       0.0.0.0/0            10.220.0.0/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> RETURN     all  --  *      *       0.0.0.0/0            10.220.2.0/24        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> RETURN     all  --  *      *       0.0.0.0/0            10.200.0.0/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
   <span style="color:#ae81ff">14</span>   <span style="color:#ae81ff">911</span> MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* ip-masq-agent: outbound traffic is subject to MASQUERADE <span style="color:#f92672">(</span>must be last in chain<span style="color:#f92672">)</span> */
</code></pre></div><p>Ok, so for any source (0.0.0.0/0) if the destination is 10.220.0.0/16 (vnet cidr) or 10.220.2.0/24 (subnet cidr) or 10.200.0.0/16 (service cidr) the traffic should go to the RETURN chain&hellip;.which basically means that it should just go out as is. However, if none of those rules hit (i.e. the traffic is not destined for the vnet, subnet or a service in the cluster) the traffic SHOULD go to the MASQUERADE chain, where we know from above that it will go through Source NAT, which will set the source IP to the node IP.</p>
<p>That&rsquo;s interesting. So only traffic within the vnet will really ever see the pod IP, which is good to know when you start thinking about Network Security Rules, network appliances, firewalls, etc.</p>
<p>I wonder if we can change those settings. It does mention an ip-masq-agent, so lets see if we can find it.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Lets check for any ip-masq pods in kube-system</span>
kubectl get pods -n kube-system -o wide|grep ip-masq
azure-ip-masq-agent-g2dsn                    1/1     Running   <span style="color:#ae81ff">0</span>          4h52m   10.220.2.4    aks-nodepool1-44430483-vmss000000   &lt;none&gt;           &lt;none&gt;
azure-ip-masq-agent-j27xx                    1/1     Running   <span style="color:#ae81ff">0</span>          4h53m   10.220.2.66   aks-nodepool1-44430483-vmss000002   &lt;none&gt;           &lt;none&gt;
azure-ip-masq-agent-t5cpl                    1/1     Running   <span style="color:#ae81ff">0</span>          4h53m   10.220.2.35   aks-nodepool1-44430483-vmss000001   &lt;none&gt;           &lt;none&gt;

<span style="color:#75715e"># Now lets see if there are configmaps we can look at</span>
kubectl get configmaps -n kube-system|grep ip-masq
azure-ip-masq-agent-config           <span style="color:#ae81ff">1</span>      4h55m
</code></pre></div><p>Yup&hellip;there it is along with a config map. Lets check that out.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get configmap azure-ip-masq-agent-config -n kube-system -o yaml
apiVersion: v1
data:
  ip-masq-agent: |-
    nonMasqueradeCIDRs:
      - 10.220.0.0/16
      - 10.220.2.0/24
      - 10.200.0.0/16
    masqLinkLocal: true
    resyncInterval: 60s
kind: ConfigMap
</code></pre></div><p>Great! There it is. So it looks like we may be able to modify the nonMasqueradeCIDRs to add some cidr blocks. Lets give it a try.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Edit the config map and add a row to the nonMasqueradeCIDRS</span>
<span style="color:#75715e"># I know...kubectl edit is evil...but we&#39;re just playing around here</span>
kubectl edit configmap azure-ip-masq-agent-config -n kube-system

<span style="color:#75715e"># Check the config</span>
kubectl get configmap azure-ip-masq-agent-config -n kube-system -o yaml
apiVersion: v1
data:
  ip-masq-agent: |-
    nonMasqueradeCIDRs:
      - 10.220.0.0/16
      - 10.220.2.0/24
      - 10.200.0.0/16
      - 10.1.0.0/16

<span style="color:#75715e"># Now lets see if that impacted our iptables</span>
sudo iptables -t nat -nvL IP-MASQ-AGENT

Chain IP-MASQ-AGENT <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
 pkts bytes target     prot opt in     out     source               destination
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> RETURN     all  --  *      *       0.0.0.0/0            10.220.0.0/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> RETURN     all  --  *      *       0.0.0.0/0            10.220.2.0/24        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> RETURN     all  --  *      *       0.0.0.0/0            10.200.0.0/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span> RETURN     all  --  *      *       0.0.0.0/0            10.1.0.0/16          /* ip-masq-agent: local traffic is not subject to MASQUERADE */
</code></pre></div><p>Yup! We now have a new cidr block that will NOT go through SNAT.</p>
<blockquote>
<p><strong>WARNING:</strong> While with the above you can make sure that traffic leaving your vnet does not go through SNAT, depending on the target and potential virtual appliances in the middle you may end up getting your traffic dropped. The overall details of a pod packet may not match what is expected from a machine, and therefor may not be treated like machine traffic. Proceed with caution.</p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>Hopefully the above helped you understand the overall role that iptables play in both the kubenet and Azure CNI network plugin deployments. As you can see, overall they&rsquo;re almost identical.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
