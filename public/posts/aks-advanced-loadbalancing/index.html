<!doctype html>
<html lang="en-us">
  <head>
    <title>AKS Advanced Load Balancing - Part 1: Azure Load Balancer // Steve Griffith</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.75.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Steve Griffth" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://www.stevegriffith.nyc/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-129005800-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="AKS Advanced Load Balancing - Part 1: Azure Load Balancer"/>
<meta name="twitter:description" content="Overview In the next few posts (yeah&hellip;I think this will require a few)..we&rsquo;re going to run through what the end to end traffic flow looks like for a packet going through an Azure Load Balancer, into a Kubernetes service, then an ingress controller and finally to a backend set of pods. In particular, I want to help clarify the routing decisions that are made at each step of the flow and how that can impact your application behavior and performance."/>
<meta name="twitter:site" content="@SteveGriffith"/>

    <meta property="og:title" content="AKS Advanced Load Balancing - Part 1: Azure Load Balancer" />
<meta property="og:description" content="Overview In the next few posts (yeah&hellip;I think this will require a few)..we&rsquo;re going to run through what the end to end traffic flow looks like for a packet going through an Azure Load Balancer, into a Kubernetes service, then an ingress controller and finally to a backend set of pods. In particular, I want to help clarify the routing decisions that are made at each step of the flow and how that can impact your application behavior and performance." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.stevegriffith.nyc/posts/aks-advanced-loadbalancing/" />
<meta property="article:published_time" content="2020-11-16T10:50:23-05:00" />
<meta property="article:modified_time" content="2020-11-16T10:50:23-05:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://www.stevegriffith.nyc/"><img class="app-header-avatar" src="/images/me.jpg" alt="Steve Griffth" /></a>
      <h1>Steve Griffith</h1>
      <p>Microsoft Cloud Native Global Black Belt. This is my personal blog, so opinions are my own.</p>
      <div class="app-header-social">
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">AKS Advanced Load Balancing - Part 1: Azure Load Balancer</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Nov 16, 2020
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          12 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://www.stevegriffith.nyc/tags/azure/">azure</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubernetes/">kubernetes</a><a class="tag" href="https://www.stevegriffith.nyc/tags/networking/">networking</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubenet/">kubenet</a><a class="tag" href="https://www.stevegriffith.nyc/tags/cni/">cni</a><a class="tag" href="https://www.stevegriffith.nyc/tags/aks/">aks</a></div></div>
    </header>
    <div class="post-content">
      <h2 id="overview">Overview</h2>
<p>In the next few posts (yeah&hellip;I think this will require a few)..we&rsquo;re going to run through what the end to end traffic flow looks like for a packet going through an Azure Load Balancer, into a Kubernetes service, then an ingress controller and finally to a backend set of pods. In particular, I want to help clarify the routing decisions that are made at each step of the flow and how that can impact your application behavior and performance.</p>
<p>In previous posts I&rsquo;ve run you through the full stack for the AKS network plugins <a href="../aks-networking-part1">Kubenet</a> and <a href="../aks-networking-part2">Azure CNI</a>. As part of that, we also ran through the traffic flows at the kernel level via <a href="../aks-networking-iptables">iptables</a>. Before we get into advanced load balancing, I strongly recommend you read through those posts to help with some of the base concepts that will come into play here.</p>
<p>Let&rsquo;s start by setting up our test cluster, and checking out the Azure Load Balancer.</p>
<h2 id="setup">Setup</h2>
<p>For the analysis we&rsquo;re going to run we&rsquo;ll need a test cluster with some resources deployed. I&rsquo;m going to start with an AKS cluster using the &lsquo;Kubenet&rsquo; network plugin. We&rsquo;ll deploy a set of simple web server pods and an nginx ingress controller. Before we create the cluster, however, we&rsquo;ll need a network.</p>
<h3 id="create-resource-group-vnet-and-subnets">Create Resource Group, Vnet and Subnets</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">RG<span style="color:#f92672">=</span>LoadBalancingLab
LOC<span style="color:#f92672">=</span>eastus
VNET_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.0.0/16&#34;</span>
KUBENET_AKS_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.1.0/24&#34;</span>
SVC_LB_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.3.0/24&#34;</span>

<span style="color:#75715e"># Create Resource Group</span>
az group create -n $RG -l $LOC

<span style="color:#75715e"># Create Vnet</span>
az network vnet create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-g $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n aksvnet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--address-prefix $VNET_CIDR

<span style="color:#75715e"># Create the Cluster Subnet</span>
az network vnet subnet create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vnet-name aksvnet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name kubenet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --address-prefix $KUBENET_AKS_CIDR

<span style="color:#75715e"># Get the Kubenet Subnet ID</span>
KUBENET_SUBNET_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az network vnet show -g $RG -n aksvnet -o tsv --query <span style="color:#e6db74">&#34;subnets[?name==&#39;kubenet&#39;].id&#34;</span><span style="color:#66d9ef">)</span>
</code></pre></div><h3 id="create-the-kubenet-aks-cluster">Create the Kubenet AKS Cluster</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">######################################</span>
<span style="color:#75715e"># Create the Kubenet AKS Cluster</span>
<span style="color:#75715e">#######################################</span>
az aks create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-g $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n kubenet-cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--network-plugin kubenet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--vnet-subnet-id $KUBENET_SUBNET_ID <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--zones <span style="color:#ae81ff">1</span> <span style="color:#ae81ff">2</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--pod-cidr <span style="color:#e6db74">&#34;10.100.0.0/16&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--service-cidr <span style="color:#e6db74">&#34;10.200.0.0/16&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--dns-service-ip <span style="color:#e6db74">&#34;10.200.0.10&#34;</span>

<span style="color:#75715e"># Get Credentials</span>
az aks get-credentials -g $RG -n kubenet-cluster
</code></pre></div><blockquote>
<p><strong>NOTE:</strong> We&rsquo;re creating this cluster using Availability Zones. While this post doesn&rsquo;t focus on zones, we will cover it in when we dig into services and ingress, so I&rsquo;m enabling it here for future use.</p>
</blockquote>
<h3 id="deploy-the-sample-app">Deploy the sample app</h3>
<p>To walk through the load balancing traffic flow we&rsquo;ll need a set of pods distributed across nodes with an Azure Load Balancer in front. For the sake of testing, since we have a 3 node cluster, lets have 2 pods and for the load balancer we&rsquo;ll set up a Kubernetes Service of type &lsquo;LoadBalancer&rsquo;. That will provision a public Azure Load Balancer for us in front of our cluster.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Create the Deployment and Service</span>
cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Service
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: testapp
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  ports:
</span><span style="color:#e6db74">  - port: 80
</span><span style="color:#e6db74">    protocol: TCP
</span><span style="color:#e6db74">    targetPort: 80
</span><span style="color:#e6db74">  selector:
</span><span style="color:#e6db74">    run: testapp
</span><span style="color:#e6db74">  type: LoadBalancer
</span><span style="color:#e6db74">---
</span><span style="color:#e6db74">apiVersion: apps/v1
</span><span style="color:#e6db74">kind: Deployment
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    run: testapp
</span><span style="color:#e6db74">  name: testapp
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  replicas: 1
</span><span style="color:#e6db74">  selector:
</span><span style="color:#e6db74">    matchLabels:
</span><span style="color:#e6db74">      run: testapp
</span><span style="color:#e6db74">  template:
</span><span style="color:#e6db74">    metadata:
</span><span style="color:#e6db74">      labels:
</span><span style="color:#e6db74">        run: testapp
</span><span style="color:#e6db74">    spec:
</span><span style="color:#e6db74">      containers:
</span><span style="color:#e6db74">      - image: nginx
</span><span style="color:#e6db74">        name: nginx
</span><span style="color:#e6db74">EOF</span>

<span style="color:#75715e"># Check out the deployment and service</span>
kubectl get svc,pods -o wide
NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP     PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>        AGE   SELECTOR
service/kubernetes   ClusterIP      10.200.0.1       &lt;none&gt;          443/TCP        17m   &lt;none&gt;
service/testapp      LoadBalancer   10.200.252.245   20.62.153.222   80:31857/TCP   36s   run<span style="color:#f92672">=</span>testapp

NAME                           READY   STATUS    RESTARTS   AGE   IP           NODE                                NOMINATED NODE   READINESS GATES
pod/testapp-6f7947bc4b-92g9l   1/1     Running   <span style="color:#ae81ff">0</span>          36s   10.100.0.4   aks-nodepool1-23454376-vmss000001   &lt;none&gt;           &lt;none&gt;
</code></pre></div><p>Great! Now we have our network, our cluster and a deployment we can start to play with. Since we create our service as &lsquo;type: LoadBalancer&rsquo; our cluster called out to Azure and created an Azure Load Balancer for us. Let&rsquo;s send some traffic to that endpoint and see how it behaves.</p>
<h3 id="azure-load-balancer-alb-to-node">Azure Load Balancer (ALB) to Node</h3>
<p>Any load balancer will have an algorithm it uses to determine where to send traffic. Some of the most common and basic are &lsquo;Round Robin&rsquo;, &lsquo;Statistic&rsquo; and &lsquo;Hash Based&rsquo;. If you take a look at the <a href="https://docs.microsoft.com/en-us/azure/load-balancer/concepts#load-balancing-algorithm">Azure docs for the ALB</a> we can see that the default algorithm used by the ALB is hash based.</p>
<p>In short the ALB creates a hash of the the following:</p>
<ul>
<li>Source IP</li>
<li>Source Port</li>
<li>Destination IP</li>
<li>Destination Port</li>
<li>Protocol</li>
</ul>
<p>Hashing of the above provides some stickiness, specifically if all of the above match, which will happen if the requests are from a common session. A common session will will obviously have the same source and destination IP and protocol, but additionally the outbound port from the source will be consistent. The client side &lsquo;TCP Keep Alive&rsquo; setting will determine if an how long that outbound port will remain active. Lets have a look.</p>
<p>To see this lets first <a href="https://docs.microsoft.com/en-us/azure/aks/ssh">setup SSH on our cluster</a> and use <a href="https://github.com/yokawasa/kubectl-plugin-ssh-jump/blob/master/README.md">ssh-jump</a> to access a node.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the managed cluster resource group and scale set names</span>
CLUSTER_RESOURCE_GROUP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az aks show --resource-group $RG --name kubenet-cluster --query nodeResourceGroup -o tsv<span style="color:#66d9ef">)</span>
SCALE_SET_NAME<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az vmss list --resource-group $CLUSTER_RESOURCE_GROUP --query <span style="color:#e6db74">&#34;[0].name&#34;</span> -o tsv<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Add your local public key to the VMSS to enable ssh access</span>
az vmss extension set  <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $CLUSTER_RESOURCE_GROUP <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vmss-name $SCALE_SET_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name VMAccessForLinux <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --publisher Microsoft.OSTCExtensions <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --version 1.4 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --protected-settings <span style="color:#e6db74">&#34;{\&#34;username\&#34;:\&#34;azureuser\&#34;, \&#34;ssh_key\&#34;:\&#34;</span><span style="color:#66d9ef">$(</span>cat ~/.ssh/id_rsa.pub<span style="color:#66d9ef">)</span><span style="color:#e6db74">\&#34;}&#34;</span>

az vmss update-instances --instance-ids <span style="color:#e6db74">&#39;*&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $CLUSTER_RESOURCE_GROUP <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name $SCALE_SET_NAME

<span style="color:#75715e"># Grab the node name for one of our pods</span>
kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP           NODE                                NOMINATED NODE   READINESS GATES
testapp-6f7947bc4b-92g9l   1/1     Running   <span style="color:#ae81ff">0</span>          26m   10.100.0.4   aks-nodepool1-23454376-vmss000001   &lt;none&gt;           &lt;none&gt;

<span style="color:#75715e"># ssh-jump to the node. Note: Sometimes it takes a minute for the jump pod to be ready, </span>
<span style="color:#75715e"># so you may need to run the command a couple times.</span>
kubectl ssh-jump aks-nodepool1-23454376-vmss000001
</code></pre></div><p>Ok, so now we&rsquo;re connected into a node that has one of our web server pods running on it. I want to see requests hitting that node network interface card and what source they&rsquo;re coming from so that I can see how traffic is being load balanced. We could use tcpdump for this, capture a pcap file and then open that in WireShark to do all kinds of fun analysis, but getting the file off of the node will be annoying. Alternatively, we could use <a href="https://github.com/eldadru/ksniff">ksniff</a> to feed the pod traffic directly into a local instance of WireShark, but I really want to see the raw traffic as close to the interface card as possible. While there are probably 100 other ways, I found <a href="https://www.wireshark.org/docs/man-pages/tshark.html">tshark</a> to be the perfect tool here. tshark will let you get all the query and filter functionality of WireShark in a local terminal, which we can run right on our node.</p>
<p>First, lets install tshark.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo apt update
sudo apt install tshark
</code></pre></div><p>Now lets take a look at the command we&rsquo;re going to run to watch traffic into our node. Check out the following and then I&rsquo;ll break it all down.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo tshark -i eth0 -f <span style="color:#e6db74">&#39;port 80&#39;</span> -Y <span style="color:#e6db74">&#34;http.request.method == &#34;</span>GET<span style="color:#e6db74">&#34; &amp;&amp; http contains YO&#34;</span> -T fields -e ip.src -e tcp.srcport -e ip.dst -e tcp.dstport -e ip.proto
</code></pre></div><p>Here are the key components of this command:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Param</th>
<th style="text-align:left">Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">-i eth0</td>
<td style="text-align:left">Select the eth0 network interface</td>
</tr>
<tr>
<td style="text-align:left">-f &lsquo;port 80&rsquo;</td>
<td style="text-align:left">This is a filter for tcpdump to just get traffic to port 80</td>
</tr>
<tr>
<td style="text-align:left">-Y &ldquo;http.request.method == &ldquo;GET&rdquo; &amp;&amp; http contains YO&rdquo;</td>
<td style="text-align:left">Since we&rsquo;re grabbing all port 80 traffic we need a way to filter it down to our specific requests. There are many ways to do this, but I chose to use the -Y flag to query out all of the GET requests that contain the word &lsquo;YO&rsquo;</td>
</tr>
<tr>
<td style="text-align:left">-T fields -e ip.src -e tcp.srcport -e ip.dst -e tcp.dstport -e ip.proto</td>
<td style="text-align:left">Display the source ip, source port, destination ip, destination port and protocol as output (6=TCP for protocol)</td>
</tr>
</tbody>
</table>
<p>If we run the above command on the Kubernetes node it should now be listening for our traffic&hellip;.and now we need to send some traffic. There are a million options here. I personally like either curl or <a href="https://github.com/rakyll/hey">hey</a>. For this test I&rsquo;ll use hey.</p>
<p>I&rsquo;ll start by sending just 10 request to see what we see come through on our node.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Send 10 web request with the message body &#39;YO&#39; to my service load balancer</span>
hey -d <span style="color:#e6db74">&#34;YO&#34;</span> -n <span style="color:#ae81ff">10</span> -c <span style="color:#ae81ff">1</span> http://20.62.153.212

<span style="color:#75715e"># Output on the AKS Node side</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">65519</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
</code></pre></div><p>Well thats interesting! I have a 3 node cluster, but it seems that ALL of my traffic came directly to my node. Knowing that we have a hash based load balancer I was hoping to see traffic hitting different nodes and bouncing over here. If you paid attention when we discussed how hash based load balancing works you probably already know what happened. The output above shows all the values that go into the hashing&hellip;and they&rsquo;re the same for EVERY request, so of course the traffic went the same way every time.</p>
<p>If I want my traffic to hit different nodes, how can I force that. I could send traffic from a bunch of different machines to change the source IP, but that&rsquo;s annoying. I cant change the destination IP or port, or the protocol. What I can change, however, is the source port. Source port is unique for each tcp session. If I can disable tcp keep alive, then every request should have it&rsquo;s own source port. Fortunately &lsquo;hey&rsquo; has a flag for that. Lets try.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Send another 10 request, but disable keepalive</span>
hey --disable-keepalive -d <span style="color:#e6db74">&#34;YO&#34;</span> -n <span style="color:#ae81ff">10</span> -c <span style="color:#ae81ff">1</span> http://20.62.153.212

<span style="color:#75715e"># Output on the AKS Node side</span>
10.220.1.6      <span style="color:#ae81ff">1790</span>  10.100.0.4    <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">49208</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4      <span style="color:#ae81ff">65333</span> 10.100.0.4    <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.6      <span style="color:#ae81ff">45617</span> 10.100.0.4    <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">49211</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4      <span style="color:#ae81ff">57109</span> 10.100.0.4    <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.6      <span style="color:#ae81ff">49180</span> 10.100.0.4    <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
71.246.222.127  <span style="color:#ae81ff">49214</span> 20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4      <span style="color:#ae81ff">62651</span> 10.100.0.4    <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.6      <span style="color:#ae81ff">22446</span> 10.100.0.4    <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
</code></pre></div><p>Ah, that looks better. Now we&rsquo;re seeing traffic more evenly distributed. I have three nodes, so some of my traffic comes direct to the node without any SNAT (thats the traffic you see with an internet ip of 71.X.X.X), the rest of the traffic you see coming from 10.220.1.4 and 10.220.1.6, which are the other nodes in the cluster. When traffic hits a node kube-proxy and iptables will take over and send that traffic along to the right place (more on this in my <a href="../aks-adv-loadbalancing-part2">next post</a>). In this case, since we only have one node with my testapp pod on it, all the other nodes will just pass the traffic to the node we&rsquo;re currently monitoring. That traffic will SNAT, which is why we only see my internet IP on traffic that the ALB sent directly to the node my pod is sitting on.</p>
<p>So what we can gleam from the above is that our traffic will be evenly distributed if it&rsquo;s evenly sourced. If we have a specific source that holds extra long tcp sessions, or if a specific source is very &lsquo;bursty&rsquo; (i.e. a bunch of traffic in a very short period of time), that traffic may make the distribution a bit unbalanced.</p>
<h3 id="sessionaffinity">SessionAffinity</h3>
<p>What if we actually want the session to be sticky? Well, Kubernetes has a solution for that. Let&rsquo;s see how that works.</p>
<p>If you want session affinity, you can set this on the service object by setting &lsquo;service.spec.sessionAffinity&rsquo; to &lsquo;ClientIP&rsquo;&hellip;.but what impact does that actually have? Does the routing algorithm for the Azure Load Balancer actually change from using a hash based distribution (Default). Lets have a look.</p>
<p>First lets check out our cluster&rsquo;s load balancer current configuration</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the Cluster Resource Group, which will contain the load balancer</span>
CLUSTER_RESOURCE_GROUP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az aks show --resource-group $RG --name kubenet-cluster --query nodeResourceGroup -o tsv<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Get the distribution mode for the Azure Load Balancer</span>
az network lb rule list -g $CLUSTER_RESOURCE_GROUP --lb-name kubernetes -o yaml|grep loadDistribution
<span style="color:#75715e"># Output</span>
loadDistribution: Default
</code></pre></div><p>As you can see above, we&rsquo;re still using the &lsquo;Default&rsquo; loadDistribution algorithm which is hash based. Lets change the service to enable sessionAffinity.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># I&#39;m going to be evil here and just kubectl edit...YOLO</span>
kubectl edit svc testapp

<span style="color:#75715e"># Change the sessionAffinity setting to &#39;ClientIP</span>
<span style="color:#75715e"># Save and exit vim...unless you&#39;ve changed your default editor</span>

<span style="color:#75715e"># On the client side, run our hey test again</span>
hey --disable-keepalive -d <span style="color:#e6db74">&#34;YO&#34;</span> -n <span style="color:#ae81ff">100</span> -c <span style="color:#ae81ff">2</span> http://20.62.153.222

<span style="color:#75715e"># Check the output on our node:</span>
10.220.1.4  <span style="color:#ae81ff">17239</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">42969</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">35003</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">57065</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">60440</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">46570</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">9558</span>  10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">26470</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">22225</span> 10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
10.220.1.4  <span style="color:#ae81ff">9299</span>  10.100.0.4  <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
</code></pre></div><p>Interesting, so now we see that ALL of our traffic is coming directly from 10.220.1.4. If I jump to another host and hit this link, I&rsquo;ll see that I take a different path.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># I jumped into the Azure Cloud Shell and sent the following 4 times</span>
curl -H <span style="color:#e6db74">&#39;Cache-Control: no-cache&#39;</span> -H <span style="color:#e6db74">&#39;YO: itme&#39;</span> http://20.62.153.222

<span style="color:#75715e"># In my tshark logs I see this</span>
104.211.53.219  <span style="color:#ae81ff">3008</span>  20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
104.211.53.219  <span style="color:#ae81ff">3009</span>  20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
104.211.53.219  <span style="color:#ae81ff">3010</span>  20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
104.211.53.219  <span style="color:#ae81ff">3011</span>  20.62.153.222 <span style="color:#ae81ff">80</span>  <span style="color:#ae81ff">6</span>
</code></pre></div><p>So did this change actually modify the Azure Load Balancer algorithm?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the Cluster Resource Group, which will contain the load balancer</span>
CLUSTER_RESOURCE_GROUP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az aks show --resource-group $RG --name kubenet-cluster --query nodeResourceGroup -o tsv<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Get the distribution mode for the Azure Load Balancer</span>
az network lb rule list -g $CLUSTER_RESOURCE_GROUP --lb-name kubernetes -o yaml|grep loadDistribution

<span style="color:#75715e">#Output</span>
loadDistribution: SourceIP
</code></pre></div><p>Indeed it did! So by modifying the kubernetes service object to have a sessionAffinity of &lsquo;ClientIP&rsquo; a call was initiated from the cluster to update the load balancing algorithm of our Azure Load Balancer from &lsquo;Default&rsquo; (hash based) to &lsquo;SourceIP&rsquo;.</p>
<h2 id="summary">Summary</h2>
<p>In this post we focused directly on the relationship between an Azure Load Balancer and an AKS cluster. We learned a few key things.</p>
<ol>
<li>
<p>As you saw, the ALB has a hash based default load balancing algorithm, which generally distributes traffic well, but you should be aware of your traffic patterns and how they may lead to hot spots in your cluster. In particular the if callers are holding long tcp sessions.</p>
</li>
<li>
<p>Along the same lines, considering that the traffic is evenly distributed, if you enable <a href="https://docs.microsoft.com/en-us/azure/aks/availability-zones">Availability Zones</a>, you could see some increased latency as traffic is routed to a node in zone 1 and then kubernetes bounces it over to a node in zone 2. We&rsquo;ll dig into this one a bit more in my next post on Service Level Load Balancing.</p>
</li>
<li>
<p>AKS will modify the distribution method of the ALB to SourceIP if you enable sessionAffinity for your Kubernetes service.</p>
</li>
</ol>
<p>I very intentionally avoided getting into kubernetes service routing, internal to the cluster, and the impact of iptables. We&rsquo;ll take a look at that in my next post. Hopefully you found this useful.</p>
<p><strong>Next:</strong> <a href="../aks-adv-loadbalancing-part2">AKS Advanced Load Balancing Part 2: Kubernetes Services</a></p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
