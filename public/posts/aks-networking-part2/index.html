<!doctype html>
<html lang="en-us">
  <head>
    <title> // Steve Griffith - Ramblings</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.75.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Steve Griffth" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://blog.stevegriffith.nyc/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-129005800-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="AKS Networking Overview - Part 2: Azure CNI Setup We&rsquo;ve been through the kubenet implementation, and now we&rsquo;re on to Azure CNI. Lets start by creating an Azure CNI based AKS cluster. We&rsquo;ve already created the Vnet and Subnets, so all we need to do is create the cluster.
Notice a few changes in the &lsquo;az aks create&rsquo; command below.
 Cluster name to &lsquo;azurecni-cluster&rsquo; Network Plugin to &lsquo;azure&rsquo; Removed the &lsquo;&ndash;pod-cidr&rsquo; flag, as pods will be attached to the subnet directly  Pod and Service CIDR Sizes As noted in our kubenet walkthrough, the options we set on cluster creation will impact the size of the pod and service cidrs."/>
<meta name="twitter:site" content="@SteveGriffith"/>

    <meta property="og:title" content="" />
<meta property="og:description" content="AKS Networking Overview - Part 2: Azure CNI Setup We&rsquo;ve been through the kubenet implementation, and now we&rsquo;re on to Azure CNI. Lets start by creating an Azure CNI based AKS cluster. We&rsquo;ve already created the Vnet and Subnets, so all we need to do is create the cluster.
Notice a few changes in the &lsquo;az aks create&rsquo; command below.
 Cluster name to &lsquo;azurecni-cluster&rsquo; Network Plugin to &lsquo;azure&rsquo; Removed the &lsquo;&ndash;pod-cidr&rsquo; flag, as pods will be attached to the subnet directly  Pod and Service CIDR Sizes As noted in our kubenet walkthrough, the options we set on cluster creation will impact the size of the pod and service cidrs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.stevegriffith.nyc/posts/aks-networking-part2/" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://blog.stevegriffith.nyc/"><img class="app-header-avatar" src="/images/me.jpg" alt="Steve Griffth" /></a>
      <h1>Steve Griffith - Ramblings</h1>
      <p>Ramblings of an Azure nerd.</p>
      <div class="app-header-social">
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title"></h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 1, 0001
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          8 min read
        </div></div>
    </header>
    <div class="post-content">
      <h1 id="aks-networking-overview---part-2-azure-cni">AKS Networking Overview - Part 2: Azure CNI</h1>
<h2 id="setup">Setup</h2>
<p>We&rsquo;ve been through the <a href="../aks-networking-part1">kubenet implementation</a>, and now we&rsquo;re on to Azure CNI. Lets start by creating an Azure CNI based AKS cluster. We&rsquo;ve already created the Vnet and Subnets, so all we need to do is create the cluster.</p>
<p>Notice a few changes in the &lsquo;az aks create&rsquo; command below.</p>
<ul>
<li>Cluster name to &lsquo;azurecni-cluster&rsquo;</li>
<li>Network Plugin to &lsquo;azure&rsquo;</li>
<li>Removed the &lsquo;&ndash;pod-cidr&rsquo; flag, as pods will be attached to the subnet directly</li>
</ul>
<h3 id="pod-and-service-cidr-sizes">Pod and Service CIDR Sizes</h3>
<p>As noted in our kubenet walkthrough, the options we set on cluster creation will impact the size of the pod and service cidrs. The pod cidr will be based on the subnet, as noted above, however the service cidr will still be defined as a parameter on our cluster creation, and will ultimately translate to the creation of an overlay network within our cluster. As mentioned previously, this cidr can be smaller than the address space needed for pods.</p>
<p>Again, as we noted in the kubenet overview, when a service of type &lsquo;LoadBalancer&rsquo; is created, you will want to <a href="https://docs.microsoft.com/en-us/azure/aks/internal-lb#specify-a-different-subnet">specify a target subnet</a> where the Azure Loadbalancer frontend will live.</p>
<h3 id="create-the-kubenet-aks-cluster">Create the Kubenet AKS Cluster</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># We&#39;ll re-use the RG and LOC, so lets set those</span>
RG<span style="color:#f92672">=</span>NetworkLab
LOC<span style="color:#f92672">=</span>eastus

<span style="color:#75715e"># Get the Azure CNI Subnet ID</span>
AZURECNI_SUBNET_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az network vnet show -g $RG -n aksvnet -o tsv --query <span style="color:#e6db74">&#34;subnets[?name==&#39;azurecni&#39;].id&#34;</span><span style="color:#66d9ef">)</span>

<span style="color:#75715e">######################################</span>
<span style="color:#75715e"># Create the Azure CNI AKS Cluster</span>
<span style="color:#75715e"># Note: We set a service cidr</span>
<span style="color:#75715e"># and dns service ip for demonstration</span>
<span style="color:#75715e"># purposes, however these are optional</span>
<span style="color:#75715e">#######################################</span>
az aks create <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-g $RG <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n azurecni-cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--network-plugin azure <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--vnet-subnet-id $AZURECNI_SUBNET_ID <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--service-cidr <span style="color:#e6db74">&#34;10.200.0.0/16&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--dns-service-ip <span style="color:#e6db74">&#34;10.200.0.10&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--enable-managed-identity

<span style="color:#75715e"># Get Credentials</span>
az aks get-credentials -g $RG -n azurecni-cluster

<span style="color:#75715e"># Deploy 3 Nginx Pods across 3 nodes</span>
kubectl apply -f nginx.yaml

<span style="color:#75715e"># View the Services and pods</span>
kubectl get svc
kubectl get pods -o wide --sort-by<span style="color:#f92672">=</span>.spec.nodeName <span style="color:#75715e"># Sorted by node name</span>
</code></pre></div><h3 id="pod-and-service-cidr-behavior">Pod and Service CIDR behavior</h3>
<p>Notice from your get svc and pods calls that, while the service ip addresses are from the service cidr we provided in cluster creation, the pods have IP addresses from the subnet cidr.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Subnet CIDR from network creation</span>
AZURECNI_AKS_CIDR<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10.220.2.0/24&#34;</span>

<span style="color:#75715e"># CIDR Values from &#39;az aks create&#39;</span>
--service-cidr <span style="color:#e6db74">&#34;10.200.0.0/16&#34;</span>
</code></pre></div><p>Fig. 1
<img src="/azure-networking/azurecnisvcpods.png" alt="Services and Pods"></p>
<p>As we did with kubenet, to dig a bit deeper, lets ssh into the node and explore the network configuration. Again, we&rsquo;ll use <a href="https://github.com/yokawasa/kubectl-plugin-ssh-jump/blob/master/README.md">ssh-jump</a>. Don&rsquo;t forget that you need to <a href="https://docs.microsoft.com/en-us/azure/aks/ssh">set up ssh access</a> first.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the managed cluster resource group and scale set names</span>
CLUSTER_RESOURCE_GROUP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az aks show --resource-group $RG --name azurecni-cluster --query nodeResourceGroup -o tsv<span style="color:#66d9ef">)</span>
SCALE_SET_NAME<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>az vmss list --resource-group $CLUSTER_RESOURCE_GROUP --query <span style="color:#e6db74">&#34;[0].name&#34;</span> -o tsv<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Add your local public key to the VMSS to enable ssh access</span>
az vmss extension set  <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $CLUSTER_RESOURCE_GROUP <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vmss-name $SCALE_SET_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name VMAccessForLinux <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --publisher Microsoft.OSTCExtensions <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --version 1.4 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --protected-settings <span style="color:#e6db74">&#34;{\&#34;username\&#34;:\&#34;azureuser\&#34;, \&#34;ssh_key\&#34;:\&#34;</span><span style="color:#66d9ef">$(</span>cat ~/.ssh/id_rsa.pub<span style="color:#66d9ef">)</span><span style="color:#e6db74">\&#34;}&#34;</span>

az vmss update-instances --instance-ids <span style="color:#e6db74">&#39;*&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --resource-group $CLUSTER_RESOURCE_GROUP <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --name $SCALE_SET_NAME

<span style="color:#75715e"># Get a node name and ssh-jump to it</span>
<span style="color:#75715e"># Make sure you jump to a node where one of your nginx pods is running</span>
kubectl get nodes
NAME                                STATUS   ROLES   AGE   VERSION
aks-nodepool1-44430483-vmss000000   Ready    agent   90m   v1.17.11
aks-nodepool1-44430483-vmss000001   Ready    agent   90m   v1.17.11
aks-nodepool1-44430483-vmss000002   Ready    agent   90m   v1.17.11

<span style="color:#75715e"># Use ssh-jump to access a node. Note that it may take a minute for the jump pod to come online</span>
kubectl ssh-jump &lt;Insert your node name&gt;

<span style="color:#75715e"># Get the docker id for the nginx pod</span>
docker ps|grep nginx
8bdb2bd78165        nginx                                          <span style="color:#e6db74">&#34;/docker-entrypoint.â€¦&#34;</span>   <span style="color:#ae81ff">26</span> minutes ago      Up <span style="color:#ae81ff">26</span> minutes                           k8s_nginx_nginx-7cf567cc7-5pvnj_default_56899928-244b-485e-846b-5302430a0c45_0
1f840366a5ea        mcr.microsoft.com/oss/kubernetes/pause:1.3.1   <span style="color:#e6db74">&#34;/pause&#34;</span>                 <span style="color:#ae81ff">26</span> minutes ago      Up <span style="color:#ae81ff">26</span> minutes                           k8s_POD_nginx-7cf567cc7-5pvnj_default_56899928-244b-485e-846b-5302430a0c45_0
</code></pre></div><p>So far, all the same as when we tested with kubenet. We have two containers because of the /pause container we mentioned in <a href="../aks-networking-part1">part 1</a>. Now lets dig into the newtork stack.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the pid for your container</span>
docker inspect --format <span style="color:#e6db74">&#39;{{ .State.Pid }}&#39;</span> 8bdb2bd78165
<span style="color:#ae81ff">6502</span>

<span style="color:#75715e"># List the network interfaces for the pid</span>
sudo nsenter -t <span style="color:#ae81ff">6502</span> -n ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen <span style="color:#ae81ff">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
14: eth0@if15: &lt;BROADCAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noqueue state UP group default qlen <span style="color:#ae81ff">1000</span>
    link/ether daðŸ†Ž42:26:64:0b brd ff:ff:ff:ff:ff:ff link-netnsid <span style="color:#ae81ff">0</span>
    inet 10.220.2.13/24 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre></div><p>Ok, so this all look familiar as well. We have an eth0@if15. This interface has an IP address from our Azure CNI subnet. Now lets look at the host interfaces to see what we have going on there. Yup&hellip;.looks the same as kubenet&hellip;mostly. We have an interface indexed at 14 named eth0@if15 in the container linked to an interface indexed at 15 on the host named azv292e1839522@if14. The naming is different (i.e. not veth&hellip;) but it&rsquo;s still a veth interface.</p>
<p>Fig 2.
<img src="/azure-networking/cni-vethlink.png" alt="veth link"></p>
<p>We can also see that our veth interface (azv292e1839522) is associated to a bridge network. In kubenet this bridge was called &lsquo;cbr0&rsquo;, and here it&rsquo;s called azure0. Lets install and run brctl like we did for kubenet.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Install the bridge-utils package</span>
sudo apt update
sudo apt install bridge-utils

<span style="color:#75715e"># Show the bridge networks on the server</span>
brctl show
bridge name bridge id           STP enabled interfaces
azure0      8000.000d3a8a50da   no          azv152f0ee345a
                                            azv3f6157ae51b
                                            azv8bdc5c14d80
                                            azv8ec44ee2325
                                            azvb1f7346f51b
                                            eth0
docker0		8000.0242e34adb87	no
</code></pre></div><p>Looks pretty much the same, but in this case eth0 is on the bridge. So eth0 is getting it&rsquo;s IP address the same way the rest of the pods are. Now lets look at the routes on the host:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get Routes</span>
route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.220.2.1      0.0.0.0         UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> azure0
10.220.2.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> azure0
172.17.0.0      0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> docker0
</code></pre></div><p>Nice! We can see that the azure0 interface has the subnet gateway IP as it&rsquo;s gateway, so azure0 interacts directly with the subnet! Also we can see that inbound traffic for our cluster address space (10.220.2.0/24) will also use the azure0 network.</p>
<p>We have the wiring from the container through to the network all sorted out. The only part we&rsquo;re missing is understanding how an IP address from an Azure subnet gets assigned, since there are multiple hosts constantly adding and dropping pods and there-by adding and dropping ips. This is where the CNI part of Azure CNI comes into play. Azure CNI is an implementation of the <a href="https://github.com/containernetworking/cni/blob/master/README.md">Container Network Interface</a> specification. Azure CNI is deployed on each node, and defined via a flag as the CNI plugin when the kubelet process starts. Additionally, CNI implementations are reponsible for providing an IPAM (IP Address Management) implementation for IP address assignment.</p>
<p>Looking at the <a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">Azure CNI docs</a> we can see that there are <a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md#logs">log files</a> for the CNI available at /var/log/azure-vnet.log. If I tail that file I can see the CNI plugin checking in from time to time (about every 5s) on the network interfaces. If I delete and re-apply my nginx deployment&hellip;now I can see all the magic flowing through that log.</p>
<p>Below is a very abbreviated version of what you&rsquo;d see in your logs, but have a look through and see if you can see what&rsquo;s going on based on how we know the overall network stack works so far.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Tail the CNI logs</span>
tail /var/log/azure-vnet.log -f
</code></pre></div><ol>
<li>Processing ADD command with args {ContainerID:116419acc6958dbaa1f80380eee6762928ead0caec83d0174fc5daa91a110897 Netns:/proc/19263/ns/net IfName:eth0 Args:IgnoreUnknown=1;K8S_POD_NAMESPACE=default;K8S_POD_NAME=nginx-7cf567cc7-bgc82;K8S_POD_INFRA_CONTAINER_ID=116419acc6958dbaa1f80380eee6762928ead0caec83d0174fc5daa91a110897 Path:/opt/cni/bin StdinData:{&ldquo;bridge&rdquo;:&ldquo;azure0&rdquo;,&ldquo;cniVersion&rdquo;:&ldquo;0.3.0&rdquo;,&ldquo;ipam&rdquo;:{&ldquo;type&rdquo;:&ldquo;azure-vnet-ipam&rdquo;},&ldquo;ipsToRouteViaHost&rdquo;:[&ldquo;169.254.20.10&rdquo;],&ldquo;mode&rdquo;:&ldquo;bridge&rdquo;,&ldquo;name&rdquo;:&ldquo;azure&rdquo;,&ldquo;type&rdquo;:&ldquo;azure-vnet&rdquo;}}</li>
<li>Read network configuration&hellip;</li>
<li>Found network azure with subnet 10.220.2.0/24&hellip;</li>
<li>Calling plugin azure-vnet-ipam ADD &hellip;.</li>
<li>Plugin azure-vnet-ipam returned result:IP:[{Version:4 Interface:<!-- raw HTML omitted --> Address:{IP:10.220.2.11 Mask:ffffff00}&hellip;.</li>
<li>Creating endpoint 116419ac-eth0</li>
<li>Creating veth pair azvb1f7346f51b azvb1f7346f51b2</li>
<li>Adding ARP reply rule for IP address 10.220.2.11/24</li>
<li>Adding static arp for IP address 10.220.2.11/24 and MAC ea:99:18:36:3e:10 in VM</li>
<li>Setting link azvb1f7346f51b2 netns /proc/19263/ns/net</li>
<li>Adding IP address 10.220.2.11/24 to link eth0</li>
</ol>
<p>The short translated version of the above is as follows:</p>
<ol>
<li>The assignment of a new pod to a node triggered an &lsquo;ADD&rsquo; call to the Azure CNI plugin on my node</li>
<li>Azure CNI checked out the network configuration and found the subnet cidr</li>
<li>Azure CNI asked the IPAM (IP Address Management) plugin to give it an available address</li>
<li>IPAM responded with 10.220.2.11</li>
<li>CNI proceeded to create the:
<ul>
<li>The veth pair</li>
<li>APR entry</li>
<li>Assignment of the veth link to the container network namespace</li>
<li>Association of the IP address with eth0</li>
</ul>
</li>
</ol>
<h3 id="finally">Finally!</h3>
<p>From all of the above we can see that, largely, the network stack is the same between kubenet and Azure CNI. Both rely on veth interfaces bound between a bridge network and the container network namespace. The difference comes in at the CNI and IPAM level. With kubenet the implementation creates an overlay network within the bridge which the IPAM uses to assign pod IP address, where with Azure CNI we&rsquo;re using the Azure CNI plugin and IPAM to handle ip assignment from the cluster/nodepool subnet rather than creating an overlay.</p>
<p>The only other thing to look at, which we touched on in part 1, is how iptables come into the picture. Let&rsquo;s dig into the iptables rules in both kubenet and Azure CNI, breifly.</p>
<h3 id="next">Next</h3>
<p><a href="../aks-networking-iptables">IPTables</a></p>
<h3 id="big-picture">Big Picture</h3>
<p>Fig 3.
<img src="/azure-networking/cni-wiring.jpeg" alt="kubenet wiring"></p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
