<!doctype html>
<html lang="en-us">
  <head>
    <title>AKS Advanced Load Balancing Part 2: Kubernetes Services // Steve Griffith</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.75.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Steve Griffth" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://www.stevegriffith.nyc/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-129005800-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="AKS Advanced Load Balancing Part 2: Kubernetes Services"/>
<meta name="twitter:description" content="Overview In my last post I focused on getting a deeper understanding of how AKS interacts with Azure, and specifically the Azure Load Balancer (ALB), to route traffic into a cluster. We walked through the default hashed based load balancing algorithm, and then showed the routing and ALB impact of using the Kubernetes service sessionAffinity mode of &lsquo;ClientIP&rsquo; to enable sticky sessions. If you haven&rsquo;t run through that, then I&rsquo;d definitely give part 1 a read before continuing here."/>
<meta name="twitter:site" content="@SteveGriffith"/>

    <meta property="og:title" content="AKS Advanced Load Balancing Part 2: Kubernetes Services" />
<meta property="og:description" content="Overview In my last post I focused on getting a deeper understanding of how AKS interacts with Azure, and specifically the Azure Load Balancer (ALB), to route traffic into a cluster. We walked through the default hashed based load balancing algorithm, and then showed the routing and ALB impact of using the Kubernetes service sessionAffinity mode of &lsquo;ClientIP&rsquo; to enable sticky sessions. If you haven&rsquo;t run through that, then I&rsquo;d definitely give part 1 a read before continuing here." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.stevegriffith.nyc/posts/aks-adv-loadbalancing-part2/" />
<meta property="article:published_time" content="2020-11-18T11:36:21-05:00" />
<meta property="article:modified_time" content="2020-11-18T11:36:21-05:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://www.stevegriffith.nyc/"><img class="app-header-avatar" src="/images/me.jpg" alt="Steve Griffth" /></a>
      <h1>Steve Griffith</h1>
      <p>Microsoft Cloud Native Global Black Belt. This is my personal blog, so opinions are my own.</p>
      <div class="app-header-social">
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">AKS Advanced Load Balancing Part 2: Kubernetes Services</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Nov 18, 2020
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          17 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://www.stevegriffith.nyc/tags/azure/">azure</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubernetes/">kubernetes</a><a class="tag" href="https://www.stevegriffith.nyc/tags/networking/">networking</a><a class="tag" href="https://www.stevegriffith.nyc/tags/kubenet/">kubenet</a><a class="tag" href="https://www.stevegriffith.nyc/tags/cni/">cni</a><a class="tag" href="https://www.stevegriffith.nyc/tags/aks/">aks</a></div></div>
    </header>
    <div class="post-content">
      <h2 id="overview">Overview</h2>
<p>In my last post I focused on getting a deeper understanding of how AKS interacts with Azure, and specifically the Azure Load Balancer (ALB), to route traffic into a cluster. We walked through the default hashed based load balancing algorithm, and then showed the routing and ALB impact of using the Kubernetes service sessionAffinity mode of &lsquo;ClientIP&rsquo; to enable sticky sessions. If you haven&rsquo;t run through that, then I&rsquo;d definitely give <a href="/posts/aks-advanced-loadbalancing/">part 1</a> a read before continuing here.</p>
<p>We&rsquo;re working our way down from an external user sending traffic through the ALB into a cluster, through an ingress controller and ultimately landing on a pod. After looking at the ALB, the next hop in our journey is a Kubernetes service. In this post we&rsquo;ll crack open iptables to see how creation and modification of a service ultimately translates to inter-cluster routing.</p>
<h2 id="services">Services</h2>
<p>I won&rsquo;t go into a full explanation of services in Kubernetes here, as the Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/">docs</a> do a good job of that on their own, but I will lay out a few key concepts relative to Azure Kubernetes Service.</p>
<ol>
<li>
<p>A service in Kubernetes provides a mechanism to expose a deployment to callers within and external to the cluster</p>
</li>
<li>
<p>Services are managed on a cluster via kube-proxy, which is run as a daemonset on your cluster (i.e. a kube-proxy pod on every node)</p>
</li>
<li>
<p>The <a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">type</a> set for the service is key in determining how that service is exposed (i.e. Cluster Internal, Private Network, Public Internet, etc)</p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/concepts/overview/components/#kube-proxy">kube-proxy</a> is responsible for interacting with the host node packet filtering implementation to set up relevant routes for resources on the cluster based on the &lsquo;type&rsquo; you set</p>
</li>
<li>
<p>A service of type &lsquo;LoadBalancer&rsquo; will trigger a call to the cloud provider to deploy a load balancer on the host cloud (ex. In AKS an ALB will be created. See <a href="/posts/aks-advanced-loadbalancing/">part 1</a> for more details on this implementation)</p>
</li>
</ol>
<h2 id="setup">Setup</h2>
<p>For this walk-through we&rsquo;re going to re-use the cluster created in <a href="/posts/aks-advanced-loadbalancing/">part 1</a>. This setup includes the following:</p>
<ul>
<li>Resource Group</li>
<li>Vnet with a subnet for the cluster</li>
<li>AKS Cluster
<ul>
<li>Network Plugin: Kubenet</li>
<li>Zones: 1 &amp; 2</li>
<li>Joined to Vnet noted above</li>
</ul>
</li>
<li>Test Application deployment with a Service of type LoadBalancer</li>
</ul>
<h2 id="alb-routing-review">ALB Routing Review</h2>
<p>As we discussed in <a href="/posts/aks-advanced-loadbalancing/">part 1</a>, the &lsquo;Default&rsquo; distribution method for an ALB is hash based. That means that it uses a hash of the source and destination IP and ports, along with the protocol, to distribute traffic. This can be modified by enabling sessionAffinity on the Service, but lets leave it to Default for now.</p>
<p>Let&rsquo;s again use <a href="https://github.com/yokawasa/kubectl-plugin-ssh-jump/blob/master/README.md">ssh-jump</a> to access a node and fire up our sweet <a href="https://www.wireshark.org/docs/man-pages/tshark.html">tshark</a> command to watch for traffic. We&rsquo;ll again use <a href="https://github.com/rakyll/hey">hey</a> to throw some requests at the cluster with tcp-keep-alive disabled.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the node name for the node running out pod</span>
kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE    IP           NODE                                NOMINATED NODE   READINESS GATES
testapp-6f7947bc4b-x7427   1/1     Running   <span style="color:#ae81ff">0</span>          12m    10.100.2.2   aks-nodepool1-23454376-vmss000001   &lt;none&gt;           &lt;none&gt;

<span style="color:#75715e"># SSH to the node</span>
kubectl ssh-jump aks-nodepool1-23454376-vmss000001

<span style="color:#75715e"># Run tshark</span>
sudo tshark -i eth0 -f <span style="color:#e6db74">&#39;port 80&#39;</span> -Y <span style="color:#e6db74">&#34;http.request.method == &#34;</span>GET<span style="color:#e6db74">&#34; &amp;&amp; http contains YO&#34;</span> -T fields -e ip.src -e tcp.srcport -e ip.dst -e tcp.dstport -e ip.proto
</code></pre></div><p>Now back on our local machine we run hey and check out the output in tshark on the AKS node</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the service external (ALB) ip</span>
kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>        AGE
kubernetes   ClusterIP      10.200.0.1      &lt;none&gt;          443/TCP        15m
testapp      LoadBalancer   10.200.105.65   20.185.96.169   80:30918/TCP   7m58s

<span style="color:#75715e"># Fire off 10 requests to the service with the --disable-keepalive flag to ensure no duplicate source ports</span>
hey --disable-keepalive -d <span style="color:#e6db74">&#34;YO&#34;</span> -n <span style="color:#ae81ff">10</span> -c <span style="color:#ae81ff">2</span> http://20.185.96.169

<span style="color:#75715e"># Output seen from tshark on the node</span>
71.246.222.12  <span style="color:#ae81ff">58796</span>   20.185.96.169   <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
10.220.1.6      <span style="color:#ae81ff">25185</span>   10.100.2.2      <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
10.220.1.4      <span style="color:#ae81ff">37406</span>   10.100.2.2      <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
10.220.1.6      <span style="color:#ae81ff">59377</span>   10.100.2.2      <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
71.246.222.12  <span style="color:#ae81ff">58799</span>   20.185.96.169   <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
10.220.1.4      <span style="color:#ae81ff">1222</span>    10.100.2.2      <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
10.220.1.6      <span style="color:#ae81ff">5473</span>    10.100.2.2      <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
71.246.222.12  <span style="color:#ae81ff">58802</span>   20.185.96.169   <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
10.220.1.4      <span style="color:#ae81ff">36522</span>   10.100.2.2      <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
10.220.1.6      <span style="color:#ae81ff">41753</span>   10.100.2.2      <span style="color:#ae81ff">80</span>      <span style="color:#ae81ff">6</span>
</code></pre></div><p>As you can see, our ALB is evenly distributing traffic such that we have some traffic coming directly at the node where our pod sits, as identified by the public internet ip of 72.X.X.X, and then we have an equal number of requests from our two other nodes at 10.220.1.4 and 10.220.1.6.</p>
<h2 id="iptables-and-kubernetes-services">iptables and Kubernetes Services</h2>
<p>So how does this traffic actually hit a node where a pod doesnt exist and then get bounced over to the right node. The answer lies in the impact of Service creation on kube-proxy and iptables. Lets have a look at the iptables rules on our node.</p>
<blockquote>
<p><strong>NOTE:</strong> We&rsquo;re going to run through some iptables commands which you have seen before if you read my post on <a href="/posts/aks-networking-iptables/">AKS Networking iptables</a>. If you haven&rsquo;t read that, it&rsquo;s probably worth a quick scan.</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># First have a look at the KUBE-SERVICES chain</span>
sudo iptables -t nat -nL KUBE-SERVICES
Chain KUBE-SERVICES <span style="color:#f92672">(</span><span style="color:#ae81ff">2</span> references<span style="color:#f92672">)</span>
target     prot opt source               destination         
KUBE-MARK-MASQ  tcp  -- !10.100.0.0/16        10.200.0.10          /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53
KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  0.0.0.0/0            10.200.0.10          /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53
KUBE-MARK-MASQ  tcp  -- !10.100.0.0/16        10.200.175.215       /* kube-system/metrics-server: cluster IP */ tcp dpt:443
KUBE-SVC-LC5QY66VUV2HJ6WZ  tcp  --  0.0.0.0/0            10.200.175.215       /* kube-system/metrics-server: cluster IP */ tcp dpt:443
KUBE-MARK-MASQ  tcp  -- !10.100.0.0/16        10.200.105.65        /* default/testapp: cluster IP */ tcp dpt:80
KUBE-SVC-K4VUERBNKSOP4S25  tcp  --  0.0.0.0/0            10.200.105.65        /* default/testapp: cluster IP */ tcp dpt:80
KUBE-FW-K4VUERBNKSOP4S25  tcp  --  0.0.0.0/0            20.185.96.169        /* default/testapp: loadbalancer IP */ tcp dpt:80
KUBE-MARK-MASQ  tcp  -- !10.100.0.0/16        10.200.70.146        /* kube-system/kubernetes-dashboard: cluster IP */ tcp dpt:443
KUBE-SVC-XGLOHA7QRQ3V22RZ  tcp  --  0.0.0.0/0            10.200.70.146        /* kube-system/kubernetes-dashboard: cluster IP */ tcp dpt:443
KUBE-MARK-MASQ  tcp  -- !10.100.0.0/16        10.200.218.147       /* kube-system/dashboard-metrics-scraper: cluster IP */ tcp dpt:8000
KUBE-SVC-O33EAQYCTNTKHSTD  tcp  --  0.0.0.0/0            10.200.218.147       /* kube-system/dashboard-metrics-scraper: cluster IP */ tcp dpt:8000
KUBE-MARK-MASQ  tcp  -- !10.100.0.0/16        10.200.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  0.0.0.0/0            10.200.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
KUBE-MARK-MASQ  udp  -- !10.100.0.0/16        10.200.0.10          /* kube-system/kube-dns:dns cluster IP */ udp dpt:53
KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  0.0.0.0/0            10.200.0.10          /* kube-system/kube-dns:dns cluster IP */ udp dpt:53
KUBE-NODEPORTS  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL

<span style="color:#75715e"># Lets filter down to just our testapp rules</span>
sudo iptables -t nat -nL KUBE-SERVICES|grep testapp
KUBE-MARK-MASQ  tcp  -- !10.100.0.0/16        10.200.105.65        /* default/testapp: cluster IP */ tcp dpt:80
KUBE-SVC-K4VUERBNKSOP4S25  tcp  --  0.0.0.0/0            10.200.105.65        /* default/testapp: cluster IP */ tcp dpt:80
KUBE-FW-K4VUERBNKSOP4S25  tcp  --  0.0.0.0/0            20.185.96.169        /* default/testapp: loadbalancer IP */ tcp dpt:80
</code></pre></div><p>If you look through the comments above we can see that there are three rules related to our &lsquo;testapp&rsquo; service.</p>
<ol>
<li>
<p><strong>KUBE-MARK-MASQ for source !10.100.0.0/16:</strong> This one is saying that any traffic from outside of the pod cidr (10.100.0.0/16) should be marked for Source NAT. A later chain will check for this mark and SNAT the packet.</p>
</li>
<li>
<p><strong>KUBE-SVC-K4VUERBNKSOP4S25 from 0.0.0.0/0 to 10.200.105.65:</strong> This rule says that any traffic (aka 0.0.0.0/0) destined for 10.200.105.65 (our service ClusterIP) should jump to the KUBE-SVC-K4VUERBNKSOP4S25 chain, which we&rsquo;ll check out in a minute</p>
</li>
<li>
<p><strong>KUBE-FW-K4VUERBNKSOP4S25 from 0.0.0.0/0 to 20.185.96.169:</strong> And finally, this rule says that any traffic (aka 0.0.0.0/0) destined for 20.185.96.169 should jump to the KUBE-FW-K4VUERBNKSOP4S25 chain, which we&rsquo;ll also look at in a minute</p>
</li>
</ol>
<p>First lets have a look at the KUBE-SVC-K4VUERBNKSOP4S25 chain.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the rules for the KUBE-SVC-K4VUERBNKSOP4S25</span>
sudo iptables -t nat -nL KUBE-SVC-K4VUERBNKSOP4S25
Chain KUBE-SVC-K4VUERBNKSOP4S25 <span style="color:#f92672">(</span><span style="color:#ae81ff">3</span> references<span style="color:#f92672">)</span>
target     prot opt source               destination         
KUBE-SEP-STQ7EMRESDHHERMG  all  --  0.0.0.0/0            0.0.0.0/0 

<span style="color:#75715e"># KUBE-SVC-K4VUERBNKSOP4S25 jumps all traffic to KUBE-SEP-STQ7EMRESDHHERMG so lets check that</span>
sudo iptables -t nat -nL KUBE-SEP-STQ7EMRESDHHERMG
Chain KUBE-SEP-STQ7EMRESDHHERMG <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
target     prot opt source               destination         
KUBE-MARK-MASQ  all  --  10.100.2.2           0.0.0.0/0           
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp to:10.100.2.2:80
</code></pre></div><p>Looking at the above, the flow is pretty simple (don&rsquo;t let the crazy chain names scare you). When traffic comes in destined for 10.200.105.65 (our test app service ClusterIP), the KUBE-SERVICES chain sends that along to the KUBE-SVC-K4VUERBNKSOP4S25 chain. KUBE-SVC-K4VUERBNKSOP4S25 sends everything it gets right along to the KUBE-SEP-STQ7EMRESDHHERMG chain. The KUBE-SEP-STQ7EMRESDHHERMG chain sends any traffic out of the the pod (ip 10.100.2.2) to KUBE-MARK-MASQ, where it will be marked for SNAT (i.e. all pod outbound traffic also goes through SNAT). Finally, the traffic hits the DNAT chain where it gets passed along to the pod IP of 10.100.2.2.</p>
<p>As you may recall from above, there was a chain for both the ClusterIP and the ExternalIP of the service; KUBE-SVC-K4VUERBNKSOP4S25 and KUBE-FW-K4VUERBNKSOP4S25. We checked out the ClusterIP chain already, so lets look at the ExternalIP chain.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Get the rules for the KUBE-FW-K4VUERBNKSOP4S25 chain</span>
sudo iptables -t nat -nL KUBE-FW-K4VUERBNKSOP4S25
Chain KUBE-FW-K4VUERBNKSOP4S25 <span style="color:#f92672">(</span><span style="color:#ae81ff">1</span> references<span style="color:#f92672">)</span>
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  0.0.0.0/0            0.0.0.0/0            /* default/testapp: loadbalancer IP */
KUBE-SVC-K4VUERBNKSOP4S25  all  --  0.0.0.0/0            0.0.0.0/0            /* default/testapp: loadbalancer IP */
KUBE-MARK-DROP  all  --  0.0.0.0/0            0.0.0.0/0            /* default/testapp: loadbalancer IP */
</code></pre></div><p>So this chain is effectively accomplishes the same thing as the ClusterIP chain with two additions.</p>
<p>First, any traffic that hits this chain will get sent to KUBE-MARK-MASQ, which will mark the packet for SNAT. That means that when the traffic hits the pod, it will have gone through SNAT to the node ip, which you can see if you run &lsquo;<code>kubectl logs &lt;podname&gt; -f</code>&rsquo; and then throw some traffic at the ExternalIP. The logs will show the node IP as the source. So, even though we saw my internet ip coming through at the host interface card with tshark, once that traffic hits this rule it will SNAT and the pod will only ever see the host ip.</p>
<p>Second, the traffic gets passed along to the KUBE-SVC-K4VUERBNKSOP4S25 chain, which is the exact same chain we discussed above. So that chain will ultimately pass the traffic along to the pod itself.</p>
<p>Finally, if the KUBE-SVC-K4VUERBNKSOP4S25 chain doesn&rsquo;t pass along the traffic, which I think will only happen if the protocol doesnt match (i.e. you send udp instead of tcp, which it&rsquo;s expecting in the DNAT rule), the KUBE-MARK-DROP rule will hit and the packet will be marked such that the KUBE-FIREWALL chain drops it. You can check out the KUBE-FIREWALL chain with the following: <code>sudo iptables -nL KUBE-FIREWALL</code></p>
<p>Btw - all of the rules we&rsquo;ve run through above are exactly the same across ALL nodes. Whoohooo! Why, though? Well, the magic lies in the very end of the chain, and the underlying network plugin. Looking above you can see that the final chain sends us to the pod IP of 10.100.2.2. From that point the host networking takes over by looking at the routes on the host. As you may recall from previous posts, if we run <code>routes -n</code> we can see where packets should next hop.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.220.1.1      0.0.0.0         UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
10.100.0.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> cbr0
10.220.1.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
168.63.129.16   10.220.1.1      255.255.255.255 UGH   <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
169.254.169.254 10.220.1.1      255.255.255.255 UGH   <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> docker0
</code></pre></div><p>On this kubenet cluster, any packet destined for my local node pod cidr of 10.100.0.0/24 will get sent to the cbr0 bridge network, otherwise it will go the way of the subnet gateway at 10.220.1.1, which will make sure it gets to the right node. Go back and check out <a href="/posts/aks-networking-part1/">AKS Networking Part 1</a> and <a href="/posts/aks-networking-part2/">AKS Networking Part 2</a> for more details.</p>
<h2 id="scaling-impact">Scaling Impact</h2>
<p>Thus far we&rsquo;ve had a deployment with only a single replica (i.e. single pod). Lets take a quick look at what happens when we scale up. To save on bytes, I&rsquo;ll isolate this down to JUST the parts from the above iptables chains that actually change.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Scale the deployment</span>
kubectl scale deployment testapp --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>

<span style="color:#75715e"># Check out the pods</span>
kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP           NODE                                NOMINATED NODE   READINESS GATES
testapp-6f7947bc4b-cq567   1/1     Running   <span style="color:#ae81ff">0</span>          16s   10.100.1.4   aks-nodepool1-23454376-vmss000002   &lt;none&gt;           &lt;none&gt;
testapp-6f7947bc4b-rgm5l   1/1     Running   <span style="color:#ae81ff">0</span>          16s   10.100.0.7   aks-nodepool1-23454376-vmss000000   &lt;none&gt;           &lt;none&gt;
testapp-6f7947bc4b-x7427   1/1     Running   <span style="color:#ae81ff">0</span>          27h   10.100.2.2   aks-nodepool1-23454376-vmss000001   &lt;none&gt;           &lt;none&gt;
</code></pre></div><p>As you can see, we now have 3 pods for our testapp, which Kubernetes distributed for us across our 3 nodes.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo iptables -t nat -nL KUBE-SVC-K4VUERBNKSOP4S25

Chain KUBE-SVC-K4VUERBNKSOP4S25 <span style="color:#f92672">(</span><span style="color:#ae81ff">3</span> references<span style="color:#f92672">)</span>
target     prot opt source               destination
KUBE-SEP-GB5ZXZKDF6JWMI74  all  --  0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.33333333349
KUBE-SEP-SHJOK7USDC7L53KY  all  --  0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000
KUBE-SEP-STQ7EMRESDHHERMG  all  --  0.0.0.0/0            0.0.0.0/0
</code></pre></div><p>As I mentioned, I&rsquo;m not going to make you read ALL of the chains. Just know that the KUBE-SVC-XXXX chain is the only one that really changed. As you can see above, we now have three KUBE-SEP-XXXX chains that will be hit using a statistical distribution of 1/3 to each. They execute in order, so if the first rule doesn&rsquo;t hit then there are only two left which switches the probability to 50%&hellip;so don&rsquo;t let that confuse you. It&rsquo;s still evenly distributing across the three backend pods. Again, as above, once the KUBE-SEP-XXXX chain is hit, then we do a DNAT to the right pod IP and the host network routing takes over to get the packet to the right host and/or bridge network.</p>
<h2 id="sessionaffinity">sessionAffinity</h2>
<p>The last thing we&rsquo;ll look at is the impact of the Kubernetes service sessionAffinity option. As we saw in <a href="/posts/aks-advanced-loadbalancing/">part 1</a>, if we set the sessionAffinity to ClientIP, the ALB gets updated to use SourceIP based distribution&hellip;but what does that do on the Kubernetes service side. Looking at the above we can see that once a packet hits iptables it will statistically distribute, so it goes to reason that a change must occur within iptables to make the pod sticky rather than using pure statistic load balancing.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Again...channel your inner villain and lets directly edit the service</span>
kubectl edit svc testapp

<span style="color:#75715e"># Change the sessionAffinity setting to &#39;ClientIP </span>

<span style="color:#75715e"># Lets throw a few curl calls at our service ip</span>
curl 20.185.96.169
curl 20.185.96.169
curl 20.185.96.169
</code></pre></div><p>Ok, things are about to get a little weird. Again, saving some bytes, I&rsquo;ll tell that the main change was to the KUBE-SVC-XXXX and KUBE-SEP-XXXX chains. Lets have a look.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo iptables -t nat -nL KUBE-SVC-K4VUERBNKSOP4S25
Chain KUBE-SVC-K4VUERBNKSOP4S25 <span style="color:#f92672">(</span><span style="color:#ae81ff">3</span> references<span style="color:#f92672">)</span>
target     prot opt source               destination
KUBE-SEP-GB5ZXZKDF6JWMI74  all  --  0.0.0.0/0            0.0.0.0/0            recent: CHECK seconds: <span style="color:#ae81ff">10800</span> reap name: KUBE-SEP-GB5ZXZKDF6JWMI74 side: source mask: 255.255.255.255
KUBE-SEP-SHJOK7USDC7L53KY  all  --  0.0.0.0/0            0.0.0.0/0            recent: CHECK seconds: <span style="color:#ae81ff">10800</span> reap name: KUBE-SEP-SHJOK7USDC7L53KY side: source mask: 255.255.255.255
KUBE-SEP-STQ7EMRESDHHERMG  all  --  0.0.0.0/0            0.0.0.0/0            recent: CHECK seconds: <span style="color:#ae81ff">10800</span> reap name: KUBE-SEP-STQ7EMRESDHHERMG side: source mask: 255.255.255.255
KUBE-SEP-GB5ZXZKDF6JWMI74  all  --  0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.33333333349
KUBE-SEP-SHJOK7USDC7L53KY  all  --  0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000
KUBE-SEP-STQ7EMRESDHHERMG  all  --  0.0.0.0/0            0.0.0.0/0
</code></pre></div><p>I told you it was going to get a little weird. To keep it simple, lets read this in reverse. The bottom 3 rules should look familiar. This is the normal statistic load balancing of pod traffic that we&rsquo;ve seen before. So if none of the top 3 rules hit, then the bottom 3 take over and we just get normal statistical load balancing sending 1/3 of the traffic to each backend pod through the KUBE-SEP-XXXX chains.</p>
<p>Now, lets look at the top 3. Looking at these, the target chains all make sense. Each rule sends us to one of our pods through a KUBE-SEP-XXXX chain. The source, destination and protocol all look normal too (all protocols, any source and any destination). What we haven&rsquo;t seen is this &lsquo;recent: CHECK&hellip;..&rsquo; stuff at the end, so lets break that down.</p>
<p>For these additional flags, it&rsquo;s a little bit easier to view in the iptables command line format, so lets just pipe iptables-save to grep and get all the rows with &lsquo;recent&rsquo;.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo iptables-save -t nat |grep recent
-A KUBE-SEP-GB5ZXZKDF6JWMI74 -p tcp -m recent --set --name KUBE-SEP-GB5ZXZKDF6JWMI74 --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.100.0.7:80
-A KUBE-SEP-SHJOK7USDC7L53KY -p tcp -m recent --set --name KUBE-SEP-SHJOK7USDC7L53KY --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.100.1.4:80
-A KUBE-SEP-STQ7EMRESDHHERMG -p tcp -m recent --set --name KUBE-SEP-STQ7EMRESDHHERMG --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.100.2.2:80
-A KUBE-SVC-K4VUERBNKSOP4S25 -m recent --rcheck --seconds <span style="color:#ae81ff">10800</span> --reap --name KUBE-SEP-GB5ZXZKDF6JWMI74 --mask 255.255.255.255 --rsource -j KUBE-SEP-GB5ZXZKDF6JWMI74
-A KUBE-SVC-K4VUERBNKSOP4S25 -m recent --rcheck --seconds <span style="color:#ae81ff">10800</span> --reap --name KUBE-SEP-SHJOK7USDC7L53KY --mask 255.255.255.255 --rsource -j KUBE-SEP-SHJOK7USDC7L53KY
-A KUBE-SVC-K4VUERBNKSOP4S25 -m recent --rcheck --seconds <span style="color:#ae81ff">10800</span> --reap --name KUBE-SEP-STQ7EMRESDHHERMG --mask 255.255.255.255 --rsource -j KUBE-SEP-STQ7EMRESDHHERMG
</code></pre></div><p>Great, now it&rsquo;s a little more clear whats going on. First of all, we can see the all of these rules have a &lsquo;-m recent&rsquo; flag. That flag is telling iptables to use the extension module called &lsquo;recent&rsquo;. If we check out the <a href="http://ipset.netfilter.org/iptables-extensions.man.html">iptables extensions</a> doc we can get more info.</p>
<p>Looking at the above noted doc we can see that the &lsquo;recent&rsquo; extension allows you to &lsquo;dynamically create a list of IP addresses and then match against that list in a few different ways&rsquo;. This extension is useful if you were doing something like rate limiting or DDoS protection (i.e. how many times have I seen this ip in the last 10 seconds), or in our case routing the traffic to a specific KUBE-SEP-XXXX or DNAT chain. There are few other flags there that contribute, so lets look at those.</p>
<table>
<thead>
<tr>
<th>Flag</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td>&ndash;name</td>
<td>Gives a name to the list. In our case each KUBE-SEP-XXXX (i.e. each target pod) will get it&rsquo;s own name</td>
</tr>
<tr>
<td>&ndash;rcheck</td>
<td>Checks if the inbound IP is already in the list</td>
</tr>
<tr>
<td>&ndash;set</td>
<td>Adds the current IP to the list</td>
</tr>
<tr>
<td>&ndash;rsource</td>
<td>Match and save the source IP to the list. This along with &ndash;rcheck and &ndash;set will make sure we check inbound IPs and add any that dont exist to the list</td>
</tr>
<tr>
<td>&ndash;mask</td>
<td>This sets the mask to be applied to the IP. In this case it&rsquo;s 255.255.255.255, meaning that the whole IP should be included</td>
</tr>
<tr>
<td>&ndash;reap &amp; &ndash;seconds</td>
<td>These two work together. This is saying that we should &lsquo;reap&rsquo; (i.e. purge) ips older than X seconds (10800 seconds in our case)</td>
</tr>
</tbody>
</table>
<p>One last thing to note here. As you can see from the iptables-save output, we are appending these rules to both KUBE-SEP-XXXX and KUBE-SVC-XXXX. This basically means that if a &lsquo;recent&rsquo; rule matches at the service level then you&rsquo;ll go to that pod again, but if not you just get passed along to the statistic load balancing and once you land on a pods KUBE-SEP-XXXX chain that chain will add you to a recent list so that next time you come through the service level chain will know where you send you.</p>
<p>Those more curious may wonder where this &lsquo;recent&rsquo; table lives. You can find it at <code>/proc/net/xt_recent/</code></p>
<p>Let&rsquo;s have a look:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># On our node</span>
cd /proc/net/xt_recent/

<span style="color:#75715e"># List the director contents</span>
ls
KUBE-SEP-GB5ZXZKDF6JWMI74  KUBE-SEP-SHJOK7USDC7L53KY  KUBE-SEP-STQ7EMRESDHHERMG

<span style="color:#75715e"># Check out the file contents</span>
cat KUBE-SEP-STQ7EMRESDHHERMG
src<span style="color:#f92672">=</span>71.246.222.12 ttl: <span style="color:#ae81ff">48</span> last_seen: <span style="color:#ae81ff">4319972437</span> oldest_pkt: <span style="color:#ae81ff">3</span> 4319971927, 4319972280, <span style="color:#ae81ff">4319972437</span>
</code></pre></div><p>Nice! So we can see there, that this KUBE-SEP-XXXX was hit from my internet IP&hellip;.so using the rule above, all of my traffic will continue to hit that chain.</p>
<h2 id="impact-of-availability-zones">Impact of Availability Zones</h2>
<p>In all of the above we haven&rsquo;t talked about the fact that we deployed this cluster using Availability Zones, so our pods are distributed between zones 1 &amp; 2. Does that actually make a difference, positive of negative?</p>
<p>Kubernetes uses the failure-domain.beta.kubernetes.io/zone node label to indicate the node zone. In all of the analysis we did above we never saw that come into play. So no, there really isn&rsquo;t any impact on routing, other than the fact that a packet could land on a node in zone 1, and then the iptables rule may statistically load balance that packet over to a node on zone 2. While the overhead of that route should be minimal, it&rsquo;s not zero.</p>
<p>The take away is that you may see some increased latency in a multi-zone deployment. I like to think of zonal deployments as solving similar problems to multi-regional deployments on a micro scale, while also introducing some of the same potential issues around data replication and latency.</p>
<p>We&rsquo;ll talk about this more when we cover ingress controllers in the next post.</p>
<h2 id="summary">Summary</h2>
<p>In this post we ran through Kubernetes services, which are next hop after the Azure Load Balancer as traffic flows from a user to a backend pod. Here are the key learning points we hit:</p>
<ol>
<li>
<p>The Kubernetes &lsquo;Service&rsquo; resource allows you to load balance traffic to several backend pods in a deployment</p>
</li>
<li>
<p>kube-proxy is responsible for translating the Kubernetes networking configuration (pods and services) into iptables rules on the host nodes</p>
</li>
<li>
<p>When a packet is sent to a node from an Azure Load Balancer destined for a pod behind a Kubernetes service it will flow through the following key iptables chains: KUBE-SERVICES &ndash;&gt; KUBE-SVC-XXXX &ndash;&gt; KUBE-SEP &ndash;&gt; DNAT</p>
</li>
<li>
<p>The KUBE-SVC-XXXX chain applies statistic load balancing to distribute traffic evenly across all backend pods</p>
</li>
<li>
<p>If sessionAffinity is set to ClientIP the KUBE-SVC-XXXX and KUBE-SEP-XXXX chains will use the iptables &lsquo;recent&rsquo; extension to maintain stickiness from a source ip to a backend pod.</p>
</li>
<li>
<p>Kubernetes service load balancing in AKS pays no mind to zonal deployments, so if your application is distributed across zones you may notice some minimal increased latency. The impact of that will vary by your traffic patterns (i.e. for very chatty systems the aggregate latency could be a factor). You should just be aware of this as you execute your load tests.</p>
</li>
</ol>
<p>Based on the above key points we can see that, even with sessionAffinity, our routing is still extremely basic. There&rsquo;s no concept of the impact of latency or any other more advanced metric on our traffic pattern. In my next post I&rsquo;ll look at ways that we can introduce more intelligent traffic routing through ingress controllers and service mesh.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
